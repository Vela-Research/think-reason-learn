"""RRF.

An interpretable ensemble framework for binary classification based on YES/NO questions
generated by LLMs.
"""

from __future__ import annotations

from typing import List, Self, Tuple, cast, Generator, Any, Literal
from typing import Iterable, AsyncGenerator
import asyncio
import logging
from copy import deepcopy
from importlib.util import find_spec
import contextlib
from uuid import uuid4
from pathlib import Path
import os

import numpy as np
import numpy.typing as npt
import pandas as pd
from pydantic import BaseModel, Field
import orjson

from think_reason_learn.core.llms import LLMChoice, llm, TokenCount
from think_reason_learn.core.exceptions import CorruptionError, DataError, LLMError
from ._prompts import num_questions_tag, CUMULATIVE_MEMORY_INSTRUCTIONS
from ._prompts import QUESTION_GEN_INSTRUCTIONS, QUESTION_ANSWER_INSTRUCTIONS
from ._types import EmbeddingModel, AnsSimilarityFunc
import re


logger = logging.getLogger(__name__)


class Questions(BaseModel):
    questions: List[str] = Field(..., description="The list of generated questions.")
    cumulative_memory: str = Field(..., description=CUMULATIVE_MEMORY_INSTRUCTIONS)


class Answer(BaseModel):
    answer: Literal["YES", "NO"]


class RRF:
    """Random Rule Forest."""

    def __init__(
        self,
        qgen_llmc: List[LLMChoice],
        qanswer_llmc: List[LLMChoice] | None = None,
        qgen_temperature: float = 0.0,
        qanswer_temperature: float = 0.0,
        llm_semaphore_limit: int = 3,
        embedding_model: EmbeddingModel = "hashed_bag_of_words",
        answer_similarity_func: AnsSimilarityFunc = "hamming",
        answer_similarity_threshold: float = 0.2,
        max_generated_questions: int = 100,
        max_samples_as_context: int = 30,
        class_ratio: Tuple[float, float] = (1.0, 1.0),
        semantic_threshold: float = 0.8,
        q_answer_update_interval: int = 10,
        seed: int = 42,
        save_path: str | Path | None = None,
        name: str | None = None,
    ):
        """Interpretable ensemble binary classifier.

        Args:
            qgen_llmc: LLMs to use for question generation, in priority order.
            qanswer_llmc: LLMs to use for answering questions, in priority order.
                If None, use qgen_llmc.
            qgen_temperature: Sampling temperature for question generation.
            qanswer_temperature: Sampling temperature for answering questions.
            llm_semaphore_limit: Max concurrent LLM calls.
            embedding_model: Embedding model to use for question generation.
            answer_similarity_func: Function to use for answer similarity.
            answer_similarity_threshold: Threshold for answer similarity.
            max_generated_questions: Maximum number of questions to generate. Max 1000
            max_samples_as_context: Number of samples used as context in a round
                of question generation. max 100 Max 100
            class_ratio: Ratio of samples to use for each class in a round
                of question generation.
            semantic_threshold: Threshold for semantic filtering.
            q_answer_update_interval: Logging interval of question answering.
            seed: Seed for random number generator.
            save_path: Directory to save checkpoints/models.
            name: Name of the forest instance.
        """
        assert max_generated_questions < 1000, "max_generated_questions must be < 1000"
        assert max_samples_as_context <= 100, "max_samples_as_context must be <= 100"

        if embedding_model != "hashed_bag_of_words":
            if not find_spec("sentence_transformers"):
                raise ImportError(
                    f"Unknown embedding model: {embedding_model}. "
                    "if a sentence_transformers model, please install it with "
                    "`pip install sentence-transformers`"
                )

        self.qgen_llmc = qgen_llmc
        self.qanswer_llmc = qanswer_llmc or qgen_llmc
        self.qgen_temperature = qgen_temperature
        self.qanswer_temperature = qanswer_temperature
        self.llm_semaphore_limit = llm_semaphore_limit
        self.embedding_model = embedding_model
        self.answer_similarity_func = answer_similarity_func
        self.answer_similarity_threshold = answer_similarity_threshold
        self.max_generated_questions = max_generated_questions
        self.max_samples_as_context = max_samples_as_context
        self.class_ratio = class_ratio
        self.semantic_threshold = semantic_threshold
        self.seed = seed
        self.save_path: Path = self._set_save_path(save_path)
        self.q_answer_update_interval = q_answer_update_interval
        self.name: str = self._get_name(name)

        self._token_usage: List[TokenCount] = []

        self.__llm_semaphore: asyncio.Semaphore | None = None
        self._qgen_instructions_template: str | None = None
        self._classes: List[str] | None = None
        self._X: pd.DataFrame | None = None
        self._y: npt.NDArray[np.str_] | None = None
        self._X_column = "data"
        self._task_description: str | None = None
        self._questions: pd.DataFrame = self._get_initial_questions_df()
        self._answers: pd.DataFrame = self._create_answers_df()

    def _get_name(self, name: str | None) -> str:
        if name is None:
            name = str(uuid4()).replace("-", "_")
            logger.debug(f"No name provided. Assigned name: {name}")

        if not re.match(r"^[a-zA-Z0-9_]+$", name):
            raise ValueError("Name must be only alphanumeric and underscores")
        return name

    def _set_save_path(self, save_path: str | Path | None) -> Path:
        if save_path is None:
            return (Path(os.getcwd()) / "gptrees").resolve()
        else:
            return Path(save_path).resolve()

    @property
    def token_usage(self) -> List[TokenCount]:
        """Get accumulated token usage across provider/model pairs."""
        return self._token_usage

    def _get_initial_questions_df(self) -> pd.DataFrame:
        """Initialize the questions dataframe."""
        return pd.DataFrame(
            {
                "question": pd.Series([], dtype=str),
                "embedding": pd.Series([], dtype=object),
                "excluded_in_semantics": pd.Series([], dtype=bool),
                "excluded_in_pred_similarity": pd.Series([], dtype=bool),
                "expert_excluded": pd.Series([], dtype=bool),
                "precision": pd.Series([], dtype=float),
                "recall": pd.Series([], dtype=float),
                "f1_score": pd.Series([], dtype=float),
                "accuracy": pd.Series([], dtype=float),
            },
            index=pd.Index([], dtype=str, name="id"),
        )

    def get_questions(self) -> pd.DataFrame:
        """Get generated questions dataframe.

        Returns:
            DataFrame containing:
                - question: Generated question text
                - embedding: Question embedding vectors
                - excluded_in_semantics: Whether excluded during semantic filtering
                - excluded_in_pred_similarity: Whether excluded during prediction
                    similarity filtering
                - expert_excluded: Whether excluded by expert
                - precision: Precision for each question
                - recall: Recall for each question
                - f1_score: F1 score for each question
                - accuracy: Accuracy for each question
        """
        return self._questions

    def _create_answers_df(self, index: pd.Index | None = None) -> pd.DataFrame:
        """Initialize the answers dataframe."""
        # A dataframe with columns as the answered questions ids (questions not excluded
        # during semantic filtering) and index as the samples indices from self._X
        return pd.DataFrame({}, index=index)

    def get_answers(self) -> pd.DataFrame:
        """Get answers dataframe.

        Returns:
            DataFrame containing:
                - Columns as the questions ids (not excluded during semantic filtering).
                - Index as the samples indices from self._X.
        """
        return self._answers

    @property
    def _llm_semaphore(self) -> asyncio.Semaphore:
        if self.__llm_semaphore is None:
            self.__llm_semaphore = asyncio.Semaphore(self.llm_semaphore_limit)
        return self.__llm_semaphore

    @property
    def question_gen_instructions_template(self) -> str | None:
        """Get the question generation instructions template."""
        return self._qgen_instructions_template

    @property
    def task_description(self) -> str | None:
        """Get the task description."""
        return self._task_description

    async def set_tasks(
        self,
        instructions_template: str | None = None,
        task_description: str | None = None,
    ) -> str:
        """Initialize question generation instructions template.

        This sets the task description for the RRF.
        Either sets a custom template or generates one from task description using LLM.
        For most users, LLM generation is recommended over custom templates.

        Args:
            instructions_template: Custom template to use. Must contain
                '<number_of_questions>' tag. If None, generates template
                from task_description using LLM.
            task_description: Description of classification task to help LLM generate
                the template.

        Returns:
            The question generation instructions template.

        Raises:
            ValueError: If template missing required tag or generation fails.
            AssertionError: If both parameters are None.
        """
        assert instructions_template is not None or task_description is not None, (
            "Either instructions_template or task_description must be provided"
        )

        if instructions_template:
            if num_questions_tag not in instructions_template:
                raise ValueError(
                    f"instructions_template must contain the tag '{num_questions_tag}' "
                    "This tag will be replaced with the number of questions to generate"
                )
            else:
                self._qgen_instructions_template = instructions_template
                return instructions_template

        async with self._llm_semaphore:
            response = await llm.respond(
                query=f"Generate YES/NO questions for:\n{task_description}",
                llm_priority=self.qgen_llmc,
                response_format=str,
                instructions=QUESTION_GEN_INSTRUCTIONS,
                temperature=self.qgen_temperature,
            )

        if not response.response:
            raise ValueError(
                "Failed to generate question generation instructions"
                "Try refining the task description or change the models "
                "for generating question generation instructions, "
                "`self.qgen_llmc`"
            )
        elif num_questions_tag not in response.response:
            raise ValueError(
                "Failed to generate a valid question generation "
                "instructions template. Please try again."
            )

        if response.average_confidence is not None:
            logger.info(
                "Generated question generation instructions with "
                f"confidence {response.average_confidence}"
            )
        else:
            logger.info(
                "Generated question generation instructions. Could not track "
                "confidence of instructions."
            )

        self._task_description = task_description
        self._qgen_instructions_template = response.response
        return response.response

    def _get_question_gen_instructions(self, num_questions: int) -> str:
        if not self._qgen_instructions_template:
            raise ValueError(
                "Question generation instructions template is not set"
                "Set the template using `set_tasks`"
            )

        return self._qgen_instructions_template.replace(
            num_questions_tag, str(num_questions)
        )

    def _sample_X(self, n: int) -> Generator[pd.DataFrame, None, None]:
        """Sample <=n from self._X without replacement."""
        if self._X is None:
            raise ValueError("X must be set")

        rng = np.random.default_rng(self.seed)
        indices = rng.permutation(self._X.shape[0])

        for i in range(0, len(indices), n):
            batch_indices = indices[i : i + n]
            yield self._X.iloc[batch_indices]

    async def _generate_questions(self) -> None:
        if self._X is None:
            raise ValueError("X must be set")

        num_llm_calls = cast(
            int, np.ceil(self._X.shape[0] / self.max_samples_as_context).astype(int)
        )
        num_questions_per_call = self.max_generated_questions // num_llm_calls
        instructions = self._get_question_gen_instructions(num_questions_per_call)
        cumulative_memory = "No cumulative memory yet. This is the first generation."
        all_questions: List[str] = []

        for sample_X in self._sample_X(self.max_samples_as_context):
            samples_str = sample_X[self._X_column].to_string(index=False)  # type: ignore
            query = f"Samples:\n{samples_str}\n\nCumulative memory: {cumulative_memory}"

            async with self._llm_semaphore:
                response = await llm.respond(
                    query=query,
                    llm_priority=self.qgen_llmc,
                    response_format=Questions,
                    instructions=instructions,
                    temperature=self.qgen_temperature,
                )

            self.token_usage.append(
                TokenCount(
                    provider=response.provider_model.provider,
                    model=response.provider_model.model,
                    value=response.total_tokens,
                )
            )
            questions = response.response
            if questions is None:
                raise LLMError(
                    "Could not generate questions. Please try "
                    "again or with a different llm."
                    f"\nQuery: {query[:200]}..."
                    f"\n\nCumulative memory: {cumulative_memory[:200]}..."
                    f"\n\nInstructions: {instructions[:200]}..."
                )
            all_questions.extend(questions.questions)
            cumulative_memory = questions.cumulative_memory

        qlen = len(all_questions)
        max_questions_id_len = 3  # 3 digits for 999 max questions
        index = [str(i).zfill(max_questions_id_len) for i in range(qlen)]

        self._questions = pd.DataFrame(
            {
                "question": all_questions,
                "embedding": [None] * qlen,
                "excluded_in_semantics": [False] * qlen,
                "excluded_in_pred_similarity": [False] * qlen,
                "expert_excluded": [False] * qlen,
                "precision": [None] * qlen,
                "recall": [None] * qlen,
                "f1_score": [None] * qlen,
                "accuracy": [None] * qlen,
            },
            index=index,  # type: ignore
        )

    def _hash_bag_of_words_emb_single(self, text: str) -> np.ndarray:
        dim = 256
        token_pattern = re.compile(r"\b\w+\b")
        vec = np.zeros(dim, dtype=np.float32)
        for token in token_pattern.findall(text.lower()):
            h = hash(token)
            idx = (h % dim) if h >= 0 else ((-h) % dim)
            sign = 1.0 if (h & 1) == 0 else -1.0
            vec[idx] += sign
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec /= norm
        return vec

    def _hash_bag_of_words_emb(self, text: List[str]) -> np.ndarray:
        return np.vstack([self._hash_bag_of_words_emb_single(q) for q in text])

    async def _sentence_transformer_emb(self, text: List[str]) -> np.ndarray:
        """Embed text using the sentence transformer."""
        from sentence_transformers import SentenceTransformer  # type: ignore

        encode_fn = cast(Any, SentenceTransformer(self.embedding_model).encode_async)
        embeddings_np = await encode_fn(
            text, convert_to_numpy=True, normalize_embeddings=True
        )
        embeddings_np = cast(np.ndarray, embeddings_np)
        return embeddings_np.astype(np.float32, copy=False)

    async def _set_questions_semantics(self) -> None:
        """Embed questions using the embedding model."""
        if self._questions.empty:
            return

        qdf = self._questions
        to_embed_positions: List[int] = []
        texts_to_embed: List[str] = []
        for pos, (_row_id, row) in enumerate(qdf.iterrows()):
            emb_any = cast(Any, row["embedding"])
            needs_emb: bool
            if isinstance(emb_any, np.ndarray):
                needs_emb = bool(emb_any.size == 0)
            else:
                needs_emb = True
            if needs_emb:
                to_embed_positions.append(pos)
                row_label = cast(str, qdf.index[pos])
                question_text = cast(str, qdf.at[row_label, "question"])  # type: ignore
                texts_to_embed.append(question_text)

        if not to_embed_positions:
            return

        embeddings: np.ndarray
        if self.embedding_model == "hashed_bag_of_words":
            embeddings = self._hash_bag_of_words_emb(texts_to_embed)
        else:
            try:
                embeddings = await self._sentence_transformer_emb(texts_to_embed)
            except Exception as e:
                raise ValueError(
                    f"Error embedding questions with {self.embedding_model} model. "
                    "Please check if the model is installed and try again.\n\n"
                    f"Error: {e}"
                )

        for i, pos in enumerate(to_embed_positions):
            row_label = cast(str, qdf.index[pos])
            self._questions.at[row_label, "embedding"] = embeddings[i]

    async def _filter_questions_on_semantics(
        self,
        semantic_threshold: float | None,
    ) -> None:
        """Filter questions on semantics."""
        if self._questions.empty:
            return

        semantic_threshold = (
            self.semantic_threshold
            if semantic_threshold is None
            else semantic_threshold
        )
        if semantic_threshold <= 0.0:
            return

        qdf = self._questions
        embeddings_list: List[npt.NDArray[np.float32]] = []
        indices: List[int] = []
        for idx, (_row_idx, row) in enumerate(qdf.iterrows()):
            emb_any = cast(Any, row["embedding"])
            if isinstance(emb_any, np.ndarray) and emb_any.size > 0:
                embeddings_list.append(emb_any)  # type: ignore
                indices.append(idx)

        if len(embeddings_list) <= 1:
            return

        emb_matrix = np.vstack(embeddings_list)

        # Normalize (defensive in case upstream did not)
        norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        emb_matrix = emb_matrix / norms

        # Greedy deduplication: keep first, exclude later items with high cosine sim
        keep_flags = np.ones(emb_matrix.shape[0], dtype=bool)
        for i in range(emb_matrix.shape[0]):
            if not keep_flags[i]:
                continue
            # Compare only with later vectors
            if i + 1 < emb_matrix.shape[0]:
                sims = emb_matrix[i + 1 :] @ emb_matrix[i]
                dup_mask = sims >= semantic_threshold
                keep_flags[i + 1 :][dup_mask] = False

        # Map back to original dataframe indices
        excluded_indices = [indices[i] for i, keep in enumerate(keep_flags) if not keep]
        if excluded_indices:
            qdf.loc[qdf.index[excluded_indices], "excluded_in_semantics"] = True

    async def _answer_single_question(
        self,
        sample_index: Any,
        sample: str,
        question_id: str,
        question: str,
    ) -> Tuple[Any, str, Answer] | None:
        """Returns sample_index, question_id, answer."""
        if self._X is None:
            raise ValueError("X must be set")

        try:
            async with self._llm_semaphore:
                response = await llm.respond(
                    llm_priority=self.qanswer_llmc,
                    query=f"Query: {question}\n\nSample: {sample}",
                    instructions=QUESTION_ANSWER_INSTRUCTIONS,
                    response_format=Answer,
                    temperature=self.qanswer_temperature,
                )
            if response.response is None:
                raise LLMError("No response from LLM")
            if response.average_confidence is not None:
                logger.debug(
                    f"Confidence: {response.average_confidence} "
                    f"Answered question: '{question}' for sample: {sample}"
                )
            else:
                logger.debug(
                    "Could not track confidence of answer. "
                    f"Answered question: '{question}' for sample: {sample}"
                )
            return sample_index, question_id, response.response

        except Exception as _:
            logger.warning(
                f"Error answering question: '{question}' for sample: {sample}",
                exc_info=True,
            )

    def _update_answers_df_columns(self) -> None:
        not_excluded_qs = self._questions[  # type: ignore
            ~self._questions["excluded_in_semantics"]
            & ~self._questions["excluded_in_pred_similarity"]
            & ~self._questions["expert_excluded"]
        ]

        missing = [q for q in not_excluded_qs.index if q not in self._answers.columns]  # type: ignore
        if missing:
            self._answers[missing] = None

    async def _answer_questions(self) -> None:
        if self._X is None or self._y is None:
            raise ValueError("X and y must be set")

        self._update_answers_df_columns()

        mask = self._answers.isna().stack()
        not_answered = cast(Iterable[Tuple[int, str]], mask[mask].index)  # type: ignore
        num_not_answered = not_answered.size  # type: ignore

        tasks: List[asyncio.Task[Tuple[int, str, Answer] | None]] = []
        try:
            for sample_index, question_id in not_answered:
                question = cast(str, self._questions.at[question_id, "question"])
                sample = cast(str, self._X.at[sample_index, self._X_column])
                tasks.append(
                    asyncio.create_task(
                        self._answer_single_question(
                            sample_index=sample_index,
                            sample=sample,
                            question_id=question_id,
                            question=question,
                        )
                    )
                )
            for idx, task in enumerate(asyncio.as_completed(tasks), start=1):
                with contextlib.suppress(asyncio.CancelledError):
                    result = await task
                    if result is None:
                        continue
                    sample_index, question_id, answer = result
                    self._answers.at[sample_index, question_id] = answer.answer.upper()

                if idx % self.q_answer_update_interval == 0:
                    logger.info(f"Answered {idx} questions out of {num_not_answered}")
        finally:
            for task in tasks:
                if not task.done():
                    task.cancel()

    def _set_questions_metrics(self) -> None:
        if self._y is None:
            return

        y_series = pd.Series(self._y, index=self._answers.index)

        true_yes = set(y_series.index[y_series == "YES"])  # type: ignore
        true_no = set(y_series.index[y_series == "NO"])  # type: ignore

        for qid in cast(List[str], self._questions.index):
            if qid not in self._answers.columns:
                continue

            col = cast(pd.Series, self._answers[qid])
            pred_yes = set(col.index[col == "YES"])  # type: ignore
            pred_no = set(col.index[col == "NO"])  # type: ignore

            tp = len(pred_yes & true_yes)
            tn = len(pred_no & true_no)
            fp = len(pred_yes & true_no)
            fn = len(true_yes - pred_yes)

            precision = tp / (tp + fp) if (tp + fp) else 0.0
            recall = tp / (tp + fn) if (tp + fn) else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else 0.0
            f1 = (
                (2 * precision * recall / (precision + recall))
                if (precision + recall)
                else 0.0
            )

            self._questions.at[qid, "precision"] = precision
            self._questions.at[qid, "recall"] = recall
            self._questions.at[qid, "f1_score"] = f1
            self._questions.at[qid, "accuracy"] = accuracy

    def _jaccard_similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        set1 = set(col1.index[col1 == "YES"])  # type: ignore
        set2 = set(col2.index[col2 == "YES"])  # type: ignore
        if not set1 and not set2:
            return 1.0
        union = set1 | set2
        return len(set1 & set2) / len(union) if union else 0.0

    def _hamming_similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        mask = col1.notna() & col2.notna()
        if not mask.any():
            return 0.0
        return float((col1[mask] == col2[mask]).mean())  # type: ignore

    def _similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        return (
            self._jaccard_similarity(col1, col2)
            if self.answer_similarity_func == "jaccard"
            else self._hamming_similarity(col1, col2)
        )

    def _filter_questions_on_pred_similarity(self) -> None:
        # Ensure answers has all question IDs as columns
        self._update_answers_df_columns()
        qids = cast(List[str], self._answers.columns.tolist())
        n = len(qids)
        keep: List[bool] = [True] * n

        for i in range(n):
            if not keep[i]:
                continue
            col_i = cast(pd.Series, self._answers[qids[i]])
            for j in range(i + 1, n):
                if not keep[j]:
                    continue
                col_j = cast(pd.Series, self._answers[qids[j]])
                sim = self._similarity(col_i, col_j)
                if sim >= self.answer_similarity_threshold:
                    keep[j] = False

        to_remove = [qids[k] for k in range(n) if not keep[k]]
        if to_remove:
            self._questions.loc[to_remove, "excluded_in_pred_similarity"] = True

    async def _build_rrf(self) -> None:
        X = self._X
        y = self._y

        if X is None or y is None:
            raise ValueError("X, y,  must be set")

        logger.info("Generating questions")
        await self._generate_questions()
        logger.info(f"Generated {self._questions.shape[0]} questions")

        logger.info("Checking questions semantics")
        await self._set_questions_semantics()

        logger.info("Filtering questions on semantics")
        await self._filter_questions_on_semantics(None)

        logger.info("Answering questions")
        await self._answer_questions()

        logger.info("Filtering questions on prediction similarity")
        self._filter_questions_on_pred_similarity()

        logger.info("Setting questions metrics")
        self._set_questions_metrics()

    def _set_data(
        self,
        X: pd.DataFrame,
        y: npt.NDArray[np.str_],
        copy_data: bool,
    ) -> None:
        if set(X.columns) != {self._X_column}:
            raise DataError(
                "X must be a pandas DataFrame with a single column"
                f"named '{self._X_column}'"
            )
        if (
            not isinstance(y, np.ndarray)  # type: ignore
            or y.ndim != 1
            or set(np.unique(y)) != {"YES", "NO"}
        ):  # type: ignore
            raise DataError(
                "y must be a numpy array of strings with one dimension of YES/NO"
            )

        if y.shape[0] != X.shape[0]:
            raise DataError("y and X must have the same number of rows")

        if copy_data:
            self._X = deepcopy(X).reset_index(drop=True)  # type: ignore
            self._y = deepcopy(y)
        else:
            self._X = X.reset_index(drop=True)  # type: ignore
            self._y = y

        self._questions = self._get_initial_questions_df()
        self._answers = self._create_answers_df(index=X.index)

    async def fit(
        self,
        X: pd.DataFrame | None = None,
        y: npt.NDArray[np.str_] | None = None,
        *,
        copy_data: bool = True,
        reset: bool = False,
    ) -> Self:
        """Fit the RRF to the data.

        Args:
            X: Training features. Required on first run or with reset=True.
            y: Training labels. Required on first run or with reset=True.
            copy_data: Whether to copy input data.
            reset: Clear existing state and restart forest generation.

        Returns:
            Self: Updated RRF.

        Raises:
            ValueError: If data requirements aren't met or invalid reset usage.
        """
        if reset:
            if X is None or y is None:
                raise ValueError("reset=True requires X and y")
            self._set_data(X, y, copy_data)
        else:
            if X is not None or y is not None:
                if self._X is not None or self._y is not None:
                    raise ValueError(
                        "Data already set on forest. Explicitly pass reset=True "
                        "to replace data and restart training."
                    )
                if X is None or y is None:
                    raise ValueError("Both X and y must be provided together")
                self._set_data(X, y, copy_data)

        if self._X is None or self._y is None:
            raise ValueError(
                "No data found on forest. Provide X and y (or reset=True with X,y)"
            )

        await self._build_rrf()
        logger.info("RRF built successfully")
        return self

    def _question_excluded(self, qid: str) -> bool:
        return (  # type: ignore
            self._questions.at[qid, "excluded_in_semantics"]
            or self._questions.at[qid, "excluded_in_pred_similarity"]
            or self._questions.at[qid, "expert_excluded"]
        )

    async def _predict(
        self, sample_index: int, sample: str
    ) -> AsyncGenerator[Tuple[Any, str, Literal["YES", "NO"]], None]:
        """Predict a single sample data point."""
        for qid in cast(List[str], self._questions.index):
            if self._question_excluded(qid):
                continue

            answer = await self._answer_single_question(
                sample_index=sample_index,
                sample=sample,
                question_id=qid,
                question=cast(str, self._questions.at[qid, "question"]),
            )
            if answer is None:
                continue
            sample_index, _, answer = answer

            yield sample_index, qid, answer.answer

    async def predict(
        self, samples: pd.DataFrame
    ) -> AsyncGenerator[Tuple[Any, str, Literal["YES", "NO"]], None]:
        """Predict labels for samples.

        Args:
            samples: Samples to predict.

        Returns:
            Generator of predictions[sample_index, question, answer]

        Raises:
            ValueError: If samples is empty or does not have the correct column.
        """
        if samples.shape[0] == 0:
            raise ValueError("samples must have at least one row")
        if set(samples.columns) != {self._X_column}:
            raise ValueError(
                f"samples must have a single column named {self._X_column}"
            )

        queue: asyncio.Queue[
            Literal["DONE"] | Tuple[Any, str, Literal["YES", "NO"]]
        ] = asyncio.Queue()

        async def worker(sample_index: Any, sample: str) -> None:
            try:
                async for record in self._predict(
                    sample_index=sample_index, sample=sample
                ):
                    await queue.put(record)
            finally:
                await queue.put("DONE")

        tasks: List[asyncio.Task[None]] = []
        try:
            for sample_index, row in samples.iterrows():
                sample = cast(str, row[self._X_column])
                tasks.append(asyncio.create_task(worker(sample_index, sample)))

            remaining = len(tasks)
            while remaining > 0:
                item = await queue.get()
                if item == "DONE":
                    remaining -= 1
                    continue
                else:
                    yield item
            return
        except asyncio.CancelledError:
            pass
        finally:
            for task in tasks:
                if not task.done():
                    task.cancel()

    async def exclude_question(self, question_id: str) -> str:
        """Set a question as excluded by the expert.

        Args:
            question_id: The id of the question to exclude.

        Returns:
            str: The question that was excluded.

        Raises:
            ValueError: If question id is not found.
        """
        if question_id not in self._questions.index:
            raise ValueError(f"Question with id {question_id} not found.")
        self._questions.at[question_id, "expert_excluded"] = True

        return f"Excluded: {self._questions.at[question_id, 'question']}"

    async def include_question(self, question: str) -> Literal[True]:
        """Add a question to the RRF.

        Args:
            question: The question to add.

        Raises:
            ValueError: If question already exists.
        """
        for qid, row in self._questions.iterrows():
            if row["question"].lower() == question.lower():  # type: ignore
                raise ValueError(f"Question '{question}' already exists with id {qid}.")

        id_ = str(int(max(self._questions.index)) + 1).zfill(3)  # type: ignore
        self._questions.at[id_, "question"] = question
        self._questions.at[id_, "excluded_in_semantics"] = False
        self._questions.at[id_, "excluded_in_pred_similarity"] = False
        self._questions.at[id_, "expert_excluded"] = False

        logger.info("Answering expert question on data")
        await self._answer_questions()

        logger.info("Adding expert question metrics")
        self._set_questions_metrics()
        return True

    def save(self, dir_path: str | Path | None = None) -> None:
        """Save model config to JSON and dataframes to CSV in a directory.

        If dir_path is None, uses `<self.save_path>/<self.name>`.
        """
        base = Path(dir_path) if dir_path is not None else (self.save_path / self.name)
        base.mkdir(parents=True, exist_ok=True)

        qdf = self._questions.copy()
        if "embedding" in qdf.columns:
            qdf["embedding"] = qdf["embedding"].apply(  # type: ignore
                lambda x: orjson.dumps(  # type: ignore
                    x.tolist() if isinstance(x, np.ndarray) else x
                ).decode()
                if x is not None
                else None
            )
        qdf.to_csv(base / "questions.csv")  # type: ignore

        self._answers.to_csv(base / "answers.csv")  # type: ignore

        if sum([self._X is not None, self._y is not None]) == 1:
            raise CorruptionError("Model state is corrupted. Either X or y is missing.")
        if self._X is not None:
            df = deepcopy(self._X)
            df["y"] = self._y
            df.to_csv(base / "training_data.csv")  # type: ignore

        manifest = {
            "name": self.name,
            "qgen_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.qgen_llmc
            ],
            "qanswer_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.qanswer_llmc
            ],
            "qgen_temperature": self.qgen_temperature,
            "qanswer_temperature": self.qanswer_temperature,
            "llm_semaphore_limit": self.llm_semaphore_limit,
            "embedding_model": self.embedding_model,
            "answer_similarity_func": self.answer_similarity_func,
            "answer_similarity_threshold": self.answer_similarity_threshold,
            "max_generated_questions": self.max_generated_questions,
            "max_samples_as_context": self.max_samples_as_context,
            "class_ratio": list(self.class_ratio),
            "semantic_threshold": self.semantic_threshold,
            "q_answer_update_interval": self.q_answer_update_interval,
            "seed": self.seed,
            "X_column": self._X_column,
            "save_path": str(self.save_path),
            "task_description": self._task_description,
            "qgen_instructions_template": self._qgen_instructions_template,
        }
        with (base / "rrf.json").open("w", encoding="utf-8") as f:
            f.write(orjson.dumps(manifest).decode())

    @classmethod
    def load(cls, dir_path: str | Path) -> "RRF":
        """Load an RRF saved by `save`."""
        base = Path(dir_path)
        with (base / "rrf.json").open("r", encoding="utf-8") as f:
            m = orjson.loads(f.read())

        inst = cls(
            qgen_llmc=m.get("qgen_llmc", []),
            qanswer_llmc=m.get("qanswer_llmc", []),
            qgen_temperature=m.get("qgen_temperature", 0.0),
            qanswer_temperature=m.get("qanswer_temperature", 0.0),
            llm_semaphore_limit=m.get("llm_semaphore_limit", 3),
            embedding_model=m.get("embedding_model", "hashed_bag_of_words"),
            answer_similarity_func=m.get("answer_similarity_func", "hamming"),
            answer_similarity_threshold=m.get("answer_similarity_threshold", 0.2),
            max_generated_questions=m.get("max_generated_questions", 100),
            max_samples_as_context=m.get("max_samples_as_context", 30),
            class_ratio=tuple(m.get("class_ratio", [1.0, 1.0])),
            semantic_threshold=m.get("semantic_threshold", 0.8),
            q_answer_update_interval=m.get("q_answer_update_interval", 10),
            seed=m.get("seed", 42),
            save_path=str(base.parent),
            name=m.get("name"),
        )

        inst._X_column = m.get("X_column", inst._X_column)
        inst._task_description = m.get("task_description")
        inst._qgen_instructions_template = m.get("qgen_instructions_template")

        q_path = base / "questions.csv"
        if q_path.exists():
            qdf = pd.read_csv(q_path, index_col=0)  # type: ignore
            if "embedding" in qdf.columns:
                qdf["embedding"] = qdf["embedding"].apply(  # type: ignore
                    lambda x: np.array(orjson.loads(x), dtype=np.float32)  # type: ignore
                    if x is not None
                    else None
                )
            for col in [
                "excluded_in_semantics",
                "excluded_in_pred_similarity",
                "expert_excluded",
            ]:
                if col in qdf.columns:
                    qdf[col] = qdf[col].astype(bool)  # type: ignore
            inst._questions = qdf

        a_path = base / "answers.csv"
        if a_path.exists():
            inst._answers = pd.read_csv(a_path, index_col=0)  # type: ignore

        training_data_path = base / "training_data.csv"
        if training_data_path.exists():
            training_data = pd.read_csv(training_data_path, index_col=0)  # type: ignore
            if "y" in training_data.columns:
                inst._y = training_data["y"].to_numpy(dtype=np.str_)  # type: ignore
                inst._X = training_data.drop(columns=["y"])  # type: ignore

        return inst
