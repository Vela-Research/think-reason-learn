"""Policy Induction.

An interpretable ensemble framework for
generated by LLMs.
"""

from __future__ import annotations

from typing import List, Self, Tuple, Generator, Any, Sequence, Union
from typing import Dict, Literal, Optional, Iterable, AsyncGenerator
from dataclasses import dataclass, asdict
import asyncio
import logging
from copy import deepcopy
from uuid import uuid4
from pathlib import Path
from os import PathLike
import os
from joblib import dump as joblib_dump
import orjson

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import fbeta_score
import numpy as np
import numpy.typing as npt
from numpy.typing import NDArray
import pandas as pd
from pydantic import BaseModel, Field

from think_reason_learn.core.llms import LLMChoice, llm, TokenCounter
from think_reason_learn.core.exceptions import DataError, LLMError
from ._prompts import (
    max_policy_num_tag,
    POLICY_GEN_INSTRUCTIONS,
    POLICY_PREDICT_INSTRUCTIONS,
)

import re

logger = logging.getLogger(__name__)


class Policies(BaseModel):
    policies: List[str] = Field(..., description="The list of generated policies.")


class Answer(BaseModel):
    answer: Literal["YES", "NO"]


@dataclass
class WeightTrainerConfig:
    """Configuration for training and optimizing ensemble weights.

    Args:
    beta : float
        Beta for F-beta score (e.g., 0.5 for F0.5).
    cv_folds : int
        Number of StratifiedKFold splits.
    Cs : Iterable[float]
        Candidate regularization strengths for logistic regression.
    threshold_grid : Iterable[float]
        Thresholds to search for best F-beta on validation folds.
    class_weight_balanced : bool
        Whether to use class_weight='balanced' in logistic regression.
    random_state : int
        Random seed for policy generation.
    """

    beta: float = 0.5
    penalty: Literal["l1", "l2"] = "l2"
    cv_folds: int = 5
    Cs: Iterable[float] = (1e-3, 1e-2, 1e-1, 1, 10, 100, 1000)
    threshold_grid: Iterable[float] = tuple(np.linspace(0.01, 0.99, 99))
    class_weight_balanced: bool = False
    random_state: int = 0


class PolicyInduction:
    """Interpretable ensemble binary classifier.

    Args:
        gen_llmc: LLMs to use for policy generation, in priority order.
        predict_llmc: LLM to use for prediction.
        config: Configuration for training and optimizing ensemble weights.
        gen_temperature: Sampling temperature for policy generation.
        predict_temperature: Sampling temperature for policy prediction.
        llm_semaphore_limit: Max concurrent LLM calls.
        max_policy_length: Maximum rows of policies to generate. Max 500
        class_ratio: Ratio of YES to NO samples to use as
            context for policy generation.
        max_samples_as_context: Number of samples used as context in a round
            of policy generation. Max 100
        p_predict_update_interval: Logging interval of policy prediction.
        save_path: Directory to save checkpoints/models.
        name: Name of the policy-induction instance.
        random_state: Random seed.
    """

    def __init__(
        self,
        gen_llmc: List[LLMChoice],
        predict_llmc: List[LLMChoice] | None = None,
        config: WeightTrainerConfig | None = None,
        gen_temperature: float = 0.0,
        predict_temperature: float = 0.0,
        llm_semaphore_limit: int = 3,
        max_policy_length: int = 200,
        class_ratio: Tuple[float, float] = (1.0, 1.0),
        max_samples_as_context: int = 10,
        p_predict_update_interval: int = 10,
        save_path: str | PathLike[str] | None = None,
        name: str | None = None,
        random_state: int = 42,
    ):
        locals_dict = deepcopy(locals())
        del locals_dict["self"]
        self._verify_input_data(**locals_dict)

        self.gen_llmc = gen_llmc
        self.predict_llmc = predict_llmc or gen_llmc
        self.gen_temperature = gen_temperature
        self.predict_temperature = predict_temperature
        self._llm_semaphore_limit = llm_semaphore_limit

        self.config: WeightTrainerConfig = self._validate_config(config)
        self.class_ratio = class_ratio
        self.max_policy_length = max_policy_length
        self.random_state = random_state
        self.max_samples_as_context = max_samples_as_context

        self.name: str = self._get_name(name)
        self.save_path: Path = self._set_save_path(save_path)
        self.p_predict_update_interval = p_predict_update_interval

        self._token_counter: TokenCounter = TokenCounter()
        self._llm_semaphore = asyncio.Semaphore(llm_semaphore_limit)  # async with
        self._pgen_instructions_template: str | None = None
        self._task_description: str | None = None

        self._X: pd.DataFrame | None = None
        self._y: npt.NDArray[np.str_] | None = None

        self._policy_memory: pd.DataFrame = self._set_initial_memory_df()
        self._threshold: float = 0.0
        self._lr: LogisticRegression | None = None
        self._validation_result: dict | None = None

    @property
    def threshold(self) -> float:
        if self._threshold is None:
            raise ValueError(
                "No threshold. " "Model has not been trained yet. Call fit() first."
            )
        return self._threshold

    @property
    def validation_result(self) -> dict:
        if self._validation_result is None:
            raise ValueError(
                "No validation result. "
                "Model has not been trained yet. Call fit() first."
            )
        return self._validation_result

    @threshold.setter
    def threshold(self, value: float) -> None:
        """Set a new threshold value."""
        if not isinstance(value, (float, int)):
            raise TypeError("Threshold must be a numeric value.")
        if not (0.0 <= float(value) <= 1.0):
            raise ValueError("Threshold must be between 0 and 1.")
        self._threshold = float(value)

    @property
    def lr(self) -> LogisticRegression:
        if self._lr is None:
            raise ValueError(
                "No lr." "Model has not been trained yet. Call fit() first."
            )
        return self._lr

    @property
    def llm_semaphore_limit(self) -> int:
        return self._llm_semaphore_limit

    @llm_semaphore_limit.setter
    def llm_semaphore_limit(self, value: int) -> None:
        self._llm_semaphore_limit = value
        self._llm_semaphore = asyncio.Semaphore(value)

    @property
    def token_usage(self) -> TokenCounter:
        """Get the token counter for the Policy Induction."""
        return self._token_counter

    @property
    def task_description(self) -> str | None:
        """Get the task description."""
        return self._task_description

    @property
    def policy_gen_instructions_template(self) -> str | None:
        """Get the policy generation instructions template."""
        return self._pgen_instructions_template

    def _verify_input_data(self, **kwargs: Any) -> None:
        val = kwargs["max_policy_length"]
        if not (0 < val < 500):
            raise ValueError("max_policy_length must be > 0 and < 500")
        val = kwargs["class_ratio"]
        if not (len(val) == 2 and all(cr > 0 for cr in val)):
            raise ValueError("class_ratio must be a tuple of two elements each > 0")
        val = kwargs["llm_semaphore_limit"]
        if not (val > 0):
            raise ValueError("llm_semaphore_limit must be > 0")
        val = kwargs["p_predict_update_interval"]
        if not (val > 0):
            raise ValueError("p_predict_update_interval must be > 0")
        val = kwargs["save_path"]
        if not (val is None or isinstance(val, (str, Path))):
            raise ValueError("save_path must be None, a string, or a Path")
        val = kwargs["name"]
        if not (val is None or isinstance(val, str)):
            raise ValueError("name must be None or a string")
        val = kwargs["gen_temperature"]
        if not (0 <= val <= 2):
            raise ValueError("gen_temperature must be >= 0 and <= 2")
        val = kwargs["predict_temperature"]
        if not (0 <= val <= 2):
            raise ValueError("predict_temperature must be >= 0 and <= 2")

    def _get_name(self, name: str | None) -> str:
        if name is None:
            name = str(uuid4()).replace("-", "_")
            logger.debug(f"No name provided. Assigned name: {name}")

        if not re.match(r"^[a-zA-Z0-9_]+$", name):
            raise ValueError("Name must be only alphanumeric and underscores")
        return name

    def _validate_config(
        self, cfg: Union[None, WeightTrainerConfig, Dict]
    ) -> WeightTrainerConfig:
        """Validate or create a WeightTrainerConfig object."""
        if cfg is None:
            return WeightTrainerConfig()

        if isinstance(cfg, WeightTrainerConfig):
            return cfg

        if isinstance(cfg, dict):
            try:
                config = WeightTrainerConfig(**cfg)
            except TypeError as e:
                raise ValueError(
                    f"Invalid config keys in dict: {e}. "
                    f"Expected fields:"
                    f"{list(WeightTrainerConfig.__annotations__.keys())}"
                )

            if config.penalty not in ("l1", "l2"):
                raise ValueError(f"penalty must be 'l1' or 'l2', got {config.penalty}")

            if config.beta <= 0:
                raise ValueError(f"beta must be positive, got {config.beta}")

            if not isinstance(config.Cs, (list, tuple, np.ndarray)):
                raise ValueError("Cs must be an iterable of floats")

            return config

        raise ValueError(
            f"config must be a WeightTrainerConfig, dict, "
            f"or None, but got {type(cfg).__name__}"
        )

    def _set_save_path(self, save_path: str | PathLike[str] | None) -> Path:
        if save_path is None:
            return (Path(os.getcwd()) / "policy_induction" / self.name).resolve()
        else:
            save_path = Path(save_path).resolve()
            if save_path.is_file():
                raise ValueError("Please provide a directory, not a file.")
            return save_path

    def _set_initial_memory_df(self) -> pd.DataFrame:
        """Initialize the policy memory DataFrame with explicit dtypes.

        Returns:
            pd.DataFrame: A DataFrame with the following columns:
                - policy (str): The text of the policy.
                - predictions (object): The corresponding binary predictions
                  for each sample in X (keeps positional alignment with X).
        """
        return pd.DataFrame(
            {
                "policy": pd.Series([], dtype=str),
                "predictions": pd.Series([], dtype=object),
            },
        )

    def get_memory(self) -> pd.DataFrame:
        """Get memory dataframe.

        Returns:
            pd.DataFrame: A DataFrame with the following columns:
                - policy (str): The text of the policy.
                - predictions (object): The corresponding binary predictions
                  for each sample in X (keeps positional alignment with X).
                - weight (float): The weight assigned to this policy, default 0.
        """
        return self._policy_memory

    async def set_task(
        self,
        task_description: str | None = None,
        instructions_template: str | None = None,
    ) -> str:
        """Initialize policy generation instructions template.

        This sets the task description for Policy Induction.
        Either sets a custom template or generates one from task description using LLM.
        For most users, LLM generation is recommended over custom templates.

        Args:
            instructions_template: Custom template to use. Must contain
                '<max_policy_length>' tag. If None, generates template
                from task_description using LLM.
            task_description: Description of classification task to help LLM generate
                the template.

        Returns:
            The policy generation instructions template.

        Raises:
            ValueError: If template missing required tag or generation fails.
            AssertionError: If both parameters are None.
        """
        assert task_description is not None, "Task_description must be provided"

        self._task_description = task_description

        if instructions_template:
            if max_policy_num_tag not in instructions_template:
                raise ValueError(
                    f"instructions_template must contain the tag "
                    f"'{max_policy_num_tag}'. "
                    "This tag will be replaced with the maximum number of policies "
                    "to generate."
                )
            else:
                self._pgen_instructions_template = instructions_template
                return instructions_template

        async with self._llm_semaphore:
            response = await llm.respond(
                query=f"Generate policies for:\n{task_description}",
                llm_priority=self.gen_llmc,
                response_format=str,
                instructions=POLICY_GEN_INSTRUCTIONS,
                temperature=self.gen_temperature,
            )
        await self._token_counter.append(
            provider=response.provider_model.provider,
            model=response.provider_model.model,
            value=response.total_tokens,
            caller="PolicyInduction._set_tasks",
        )
        if not response.response:
            raise ValueError(
                "Failed to generate policy generation instructions. "
                "Try refining the task description or change the models "
                "for generating policy generation instructions (self.gen_llmc)."
            )
        elif max_policy_num_tag not in response.response:
            raise ValueError(
                "Failed to generate a valid policy generation "
                "instructions template. Please try again."
            )

        if response.average_confidence is not None:
            logger.info(
                "Generated policy generation instructions with "
                f"confidence {response.average_confidence}"
            )
        else:
            logger.info(
                "Generated policy generation instructions. Could not track "
                "confidence of instructions."
            )

        self._pgen_instructions_template = response.response
        return response.response

    def _set_data(self, X: pd.DataFrame, y: Sequence[str], copy_data: bool) -> None:
        if not all(isinstance(item, str) for item in y):  # type: ignore
            raise DataError("y must be a sequence of strings")

        if len(y) != X.shape[0]:
            raise DataError("y and X must have the same number of rows")

        if set(np.unique(y)) != {"YES", "NO"}:
            raise DataError(
                "y must be a sequence of only 'YES' or 'NO' values (case insensitive)"
            )

        y_array = np.array([yi.upper() for yi in y], dtype=np.str_)

        if copy_data:
            self._X = deepcopy(X).reset_index(drop=True)  # type: ignore
            self._y = deepcopy(y_array)
        else:
            self._X = X.reset_index(drop=True)  # type: ignore
            self._y = y_array

    def _get_policy_gen_instructions(self):
        if not self._pgen_instructions_template:
            raise ValueError(
                "Policy generation instructions template is not set. "
                "Set the template using `set_tasks`."
            )

        return self._pgen_instructions_template.replace(
            max_policy_num_tag, str(self.max_policy_length)
        )

    def _sample(self, n: int) -> Generator[pd.DataFrame, None, None]:
        if self._X is None or self._y is None:
            raise ValueError("X and y must be set")

        rng = np.random.default_rng(self.random_state)
        yes_idx = rng.permutation(np.where(self._y == "YES")[0])
        no_idx = rng.permutation(np.where(self._y == "NO")[0])

        p_yes = self.class_ratio[0] / sum(self.class_ratio)
        want_yes = int(round(n * p_yes))
        want_no = n - want_yes

        taken_y = 0
        taken_n = 0
        len_yes = len(yes_idx)
        len_no = len(no_idx)

        while taken_y < len_yes or taken_n < len_no:
            take_y = min(want_yes, len_yes - taken_y)
            take_n = min(want_no, len_no - taken_n)

            if take_y == 0 and take_n == 0:
                break

            # Fill from remaining if one is short
            if take_y + take_n < n:
                rem = n - take_y - take_n
                if len_yes - taken_y > 0:
                    extra = min(rem, len_yes - taken_y - take_y)
                    take_y += extra
                    rem -= extra
                if rem > 0 and len_no - taken_n > 0:
                    take_n += min(rem, len_no - taken_n - take_n)

            batch = np.concatenate(
                [
                    yes_idx[taken_y : taken_y + take_y],
                    no_idx[taken_n : taken_n + take_n],
                ]
            )
            if batch.size == 0:
                break

            rng.shuffle(batch)
            batch_df = self._X.iloc[batch].copy()  # type: ignore
            batch_df["y"] = self._y[batch]
            yield batch_df

            taken_y += take_y
            taken_n += take_n

    async def _generate_policies(self) -> None:
        if self._X is None:
            raise ValueError("X must be set")

        if len(self._policy_memory) > 0:
            logger.info(
                f"Skip generate policy: Policy memory already exists with "
                f"{len(self._policy_memory)} records.\n"
                f"{self._policy_memory}"
            )
            return

        instructions = self._get_policy_gen_instructions()
        all_policies: List[str] = []
        task_description = self.task_description

        for sample_df in self._sample(self.max_samples_as_context):
            samples_str = ""
            for each_sample in sample_df.to_dict(orient="records"):  # type: ignore
                sample_str = "\n".join(
                    [f"{col}: {val}" for col, val in each_sample.items()]  # type: ignore
                )
                samples_str += f"\n{sample_str};"

            policy_str = "\n".join(all_policies)

            query = (
                "TASK DESCRIPTION:\n"
                f"{task_description}\n\n"
                "EXISTING POLICIES:\n"
                f"{policy_str}\n\n"
                "SAMPLES:\n"
                f"{samples_str}\n\n"
            )
            async with self._llm_semaphore:
                response = await llm.respond(
                    query=query,
                    llm_priority=self.gen_llmc,
                    response_format=Policies,
                    instructions=instructions,
                    temperature=self.gen_temperature,
                )

            await self._token_counter.append(
                provider=response.provider_model.provider,
                model=response.provider_model.model,
                value=response.total_tokens,
                caller="PolicyInduction._generate_policies",
            )

            policies = response.response
            if policies is None:
                raise LLMError(
                    "Could not generate policies. Please try "
                    "again or with a different llm."
                    f"\nQuery: {query[:200]}..."
                    f"\n\nInstructions: {instructions[:200]}..."
                )
            all_policies = policies.policies
            logger.info(f"Generated policies: {len(policies.policies)}")

        policy_lenth = len(all_policies)

        self._policy_memory = pd.DataFrame(
            {
                "policy": all_policies,
                "predictions": [None] * policy_lenth,
            }
        )

    def _fix_memory_prediction(self) -> None:
        """Ensure all policy predictions are valid. If not, set to None."""
        temp_memory = self._policy_memory.dropna(subset=["policy"]).copy()
        expected_rows = len(self._X) if self._X is not None else 0

        for idx in temp_memory.index:
            val = temp_memory.at[idx, "predictions"]
            if val is None:
                continue

            if isinstance(val, pd.Series):
                if len(val) != expected_rows:
                    temp_memory.at[idx, "predictions"] = None
            else:
                temp_memory.at[idx, "predictions"] = None

        self._policy_memory = temp_memory

    async def _score_single_policy(
        self,
        policy: str,
        samples: pd.DataFrame,
        token_counter: TokenCounter,  # type:
    ) -> Optional[pd.Series]:
        if samples is None or len(samples) == 0:
            return None

        results: Dict[Any, str | None] = {}

        completion_queue: asyncio.Queue[None] = asyncio.Queue()

        async def worker(row_idx: Any, row: pd.Series) -> None:
            nonlocal results
            try:
                sample_str = "\n".join(f"{col}: {row[col]}" for col in row.index)

                task_description = self.task_description

                query = (
                    f"Task description:\n{task_description}\n\n"
                    f"Policy:\n{policy}\n\n"
                    f"Sample:\n{sample_str}\n\n"
                )

                async with self._llm_semaphore:
                    response = await llm.respond(
                        query=query,
                        llm_priority=self.predict_llmc,
                        instructions=POLICY_PREDICT_INSTRUCTIONS,
                        response_format=Answer,
                        temperature=self.predict_temperature,
                    )

                await token_counter.append(
                    provider=response.provider_model.provider,
                    model=response.provider_model.model,
                    value=response.total_tokens,
                    caller="Policy Induction._score_single_policy",
                )
                if response.response is None:
                    raise LLMError("No response from LLM")

                if response.average_confidence is not None:
                    logger.debug(
                        f"Confidence: {response.average_confidence} "
                        f"Policy: '{policy}' for sample: {sample_str}"
                    )
                else:
                    logger.debug(
                        "Could not track confidence of answer. "
                        f"Policy: '{policy}' for sample: {sample_str}"
                    )

                txt = str(response.response.answer).strip().upper()
                txt = txt.strip().strip('".,;:')  # light cleanup

                if txt in {"YES", "NO"}:
                    results[row_idx] = txt
                else:
                    if "YES" in txt:
                        results[row_idx] = "YES"
                    elif "NO" in txt:
                        results[row_idx] = "NO"
                    else:
                        logger.warning(
                            "Unexpected model output for sample %s: %r",
                            row_idx,
                            response,
                        )
                        results[row_idx] = None

            except asyncio.CancelledError:
                pass
            except Exception:
                logger.warning(
                    "Error in PolicyInduction worker prediction", exc_info=True
                )
                results[row_idx] = None
            finally:
                completion_queue.put_nowait(None)

        it = iter(samples.iterrows())
        in_flight = 0

        try:
            async with asyncio.TaskGroup() as tg:
                for _ in range(self.llm_semaphore_limit):
                    try:
                        idx, row = next(it)
                    except StopIteration:
                        break
                    tg.create_task(worker(idx, row))
                    in_flight += 1

                while in_flight > 0:
                    await completion_queue.get()
                    in_flight -= 1

                    try:
                        idx, row = next(it)
                    except StopIteration:
                        continue
                    tg.create_task(worker(idx, row))
                    in_flight += 1
        finally:
            if len(results) == 0:
                return None

            out = pd.Series([None] * len(samples), index=samples.index, dtype="object")
            for k, v in results.items():
                if k in out.index:
                    out.at[k] = v

            return out

    async def _score_policies(self) -> None:
        if self._X is None or self._y is None:
            raise ValueError("X and y must be set")

        self._fix_memory_prediction()

        temp_memory = self._policy_memory.copy(deep=True)

        not_scored_idx = temp_memory.index[temp_memory["predictions"].isna()]
        num_not_scored = not_scored_idx.size

        predictions_buffer: Dict[Any, Any] = {}

        completions = 0

        for policy_index in not_scored_idx:
            try:
                pdf = temp_memory
                policy = str(pdf.at[policy_index, "policy"])
                samples = self._X

                result = await self._score_single_policy(
                    policy=policy,
                    samples=samples,
                    token_counter=self._token_counter,
                )

                if result is not None:
                    predictions_buffer[policy_index] = result

            except asyncio.CancelledError:
                pass
            except Exception:
                logger.warning(
                    "Error in PolicyInduction worker scoring policy", exc_info=True
                )
            finally:
                completions += 1
                if completions % self.p_predict_update_interval == 0:
                    logger.info(
                        f"Scored {completions} policies out of {num_not_scored}"
                    )

                for idx, pred in predictions_buffer.items():
                    temp_memory.at[idx, "predictions"] = pred
                self._policy_memory = temp_memory

        for idx, pred in predictions_buffer.items():
            temp_memory.at[idx, "predictions"] = pred

        self._policy_memory = temp_memory

    def _check_memory(self, require_predictions: bool = False) -> None:
        self._fix_memory_prediction()
        pm = self._policy_memory
        if pm is None or not isinstance(pm, pd.DataFrame):
            raise ValueError("Invalid _policy_memory: not a pandas DataFrame")

        required = {"policy", "predictions"}
        missing = required - set(pm.columns)
        if missing:
            raise ValueError(
                f"Invalid _policy_memory: " f"missing columns {sorted(missing)}"
            )

        pm = pm.copy()
        is_policy_str = pm["policy"].apply(lambda x: isinstance(x, str))

        if require_predictions:
            is_pred_series = pm["predictions"].apply(lambda x: isinstance(x, pd.Series))
            pm = pm[is_policy_str & is_pred_series]
        else:
            pm = pm[is_policy_str]

        if len(pm) == 0:
            raise ValueError("Invalid _policy_memory: empty after filtering")

        self._policy_memory = pm  # type: ignore

    def _build_feature_matrix(
        self,
    ) -> Tuple[np.ndarray, np.ndarray, pd.Index, np.ndarray]:
        self._check_memory()
        pm = self._policy_memory

        sample_index = self._X.index  # type: ignore
        X_df: pd.DataFrame = pd.DataFrame(index=sample_index)
        col_names: list[str] = []

        for idx, row in pm.iterrows():
            preds = row["predictions"]
            mapped = preds.map(  # type: ignore
                lambda v: 1 if (v == 1 or str(v).upper() == "YES") else 0
            )
            col_name = str(idx)
            X_df[col_name] = mapped.reindex(sample_index).astype(np.float32)
            col_names.append(col_name)

        y_series: pd.Series = pd.Series(self._y, index=sample_index)
        y_num = y_series.map(lambda v: 1 if (v == 1 or str(v).upper() == "YES") else 0)

        X_df = X_df.fillna(0.0).astype(np.float32)

        return (
            X_df.values,  # np.ndarray, float32
            y_num.astype(int).to_numpy(),  # np.ndarray, int
            sample_index,  # pd.Index
            np.array(col_names, dtype=str),  # np.ndarray[str]
        )

    def _fit_weights(self) -> None:
        cfg = self.config
        X, y, _, col_names = self._build_feature_matrix()

        n_samples, n_models = X.shape
        logger.info(f"Fitting weights: {n_samples} samples Ã— {n_models} models")

        self._feature_order_ = np.array(col_names, dtype=str)
        self._policy_pos_ = {name: i for i, name in enumerate(self._feature_order_)}
        self._n_features_ = len(self._feature_order_)

        skf = StratifiedKFold(
            n_splits=cfg.cv_folds, shuffle=True, random_state=cfg.random_state
        )
        best_C, best_cv_score, best_details = None, -np.inf, None

        for C in cfg.Cs:
            fold_scores, fold_thresholds, fold_coefs = [], [], []
            for tr_idx, val_idx in skf.split(X, y):
                X_tr, X_val = X[tr_idx], X[val_idx]
                y_tr, y_val = y[tr_idx], y[val_idx]

                lr = LogisticRegression(
                    C=C,
                    penalty=cfg.penalty,
                    solver="liblinear",
                    max_iter=500,
                    class_weight="balanced" if cfg.class_weight_balanced else None,
                    random_state=cfg.random_state,
                )
                lr.fit(X_tr, y_tr)
                probs = lr.predict_proba(X_val)[:, 1]
                best_f, best_t = -1.0, 0.5
                for t in cfg.threshold_grid:
                    preds = (probs >= t).astype(int)
                    f = fbeta_score(y_val, preds, beta=cfg.beta, zero_division=0)  # type: ignore[arg-type]
                    if f > best_f:
                        best_f, best_t = f, t
                fold_scores.append(best_f)
                fold_thresholds.append(best_t)
                fold_coefs.append(lr.coef_.ravel())

            mean_f = float(np.mean(fold_scores)) if fold_scores else -np.inf
            logger.info(f"C={C} mean F{cfg.beta}={mean_f:.5f}")
            if mean_f > best_cv_score:
                best_C, best_cv_score = C, mean_f
                best_details = {
                    "fold_scores": fold_scores,
                    "fold_thresholds": fold_thresholds,
                    "fold_coefs": fold_coefs,
                }

        final_lr = LogisticRegression(
            C=best_C,  # type: ignore
            penalty=cfg.penalty,
            solver="liblinear",
            max_iter=1000,
            class_weight="balanced" if cfg.class_weight_balanced else None,
            random_state=cfg.random_state,
        )
        final_lr.fit(X, y)

        thresholds = best_details.get("fold_thresholds", []) if best_details else []
        rec_threshold = float(np.median(thresholds)) if len(thresholds) > 0 else 0.5

        result = {
            "best_C": best_C,
            "avg_cv_fbeta": best_cv_score,
            "beta": cfg.beta,
            "thresholds_per_fold": thresholds,
            "recommended_threshold": rec_threshold,
            "config": asdict(cfg),
        }
        self._validation_result = result
        self.threshold = rec_threshold
        self._lr = final_lr

        assert self._lr.coef_.shape[1] == self._n_features_, (
            f"LR expects {self._lr.coef_.shape[1]} features, "
            f"but _n_features_={self._n_features_}"
        )

        logger.info(
            f"Fit complete. Best C={best_C}, Best CV score={best_cv_score}. "
            f"Recommended threshold={rec_threshold}"
        )

    async def _build_policyinduction(self) -> None:
        X = self._X
        y = self._y

        if X is None or y is None:
            raise ValueError("X, y must be set")

        logger.info("Generating Policies")
        await self._generate_policies()

        logger.info("Scoring Policies")
        await self._score_policies()

        logger.info("Setting policy weight")
        self._fit_weights()

    async def fit(
        self,
        X: pd.DataFrame | None = None,
        y: Sequence[str] | None = None,
        *,
        copy_data: bool = True,
        reset: bool = False,
    ) -> Self:
        """Fit the PolicyInduction to the data.

        Args:
            X: Training features. Required on first run or with reset=True.
            y: Training labels. Required on first run or with reset=True.
            copy_data: Whether to copy input data.
            reset: Clear existing state and restart policy generation.

        Returns:
            Self: Updated PolicyInduction.

        Raises:
            ValueError: If data requirements aren't met or invalid reset usage.
        """
        if reset:
            if X is None or y is None:
                raise ValueError("reset=True requires X and y")
            self._set_data(X, y, copy_data)
            self._policy_memory = self._set_initial_memory_df()

        else:
            if X is not None or y is not None:
                if X is None or y is None:
                    raise ValueError("Both X and y must be provided together")
                self._set_data(X, y, copy_data)

        if self._X is None or self._y is None:
            raise ValueError(
                "No data found. Provide X and y " "(or reset=True with X,y)"
            )

        await self._build_policyinduction()
        logger.info("PolicyInductioin settled successfully")
        return self

    def _lr_predict(self, raw: NDArray) -> Literal["YES", "NO"]:
        if (
            getattr(self, "_lr", None) is None
            or getattr(self, "threshold", None) is None
        ):
            raise RuntimeError("Model not fitted. " "Call fit() before predict().")
        vec = np.asarray(raw, dtype=float).reshape(1, -1)
        if vec.shape[1] != getattr(self, "_n_features_", vec.shape[1]):
            raise RuntimeError(
                f"Feature size mismatch: got {vec.shape[1]}, "
                f"expected {self._n_features_}"
            )
        prob = self._lr.predict_proba(vec)[0, 1]  # type: ignore[attr-defined]
        return "YES" if prob >= float(self.threshold) else "NO"

    async def _predict_single(
        self,
        sample_index: int,
        sample: str,
        token_counter: TokenCounter,
    ) -> Tuple[Any, NDArray, Literal["YES", "NO"]]:
        if not hasattr(self, "_feature_order_") or getattr(self, "_lr", None) is None:
            raise RuntimeError("Model not fitted. Call fit() before predict().")

        policies = self._policy_memory["policy"]  # Series: index -> policy_text
        policies = policies.copy()
        policies.index = policies.index.map(str)
        policies = policies.reindex(self._feature_order_)

        results = np.zeros(len(self._feature_order_), dtype=float)

        completion_queue: asyncio.Queue[None] = asyncio.Queue()

        async def worker(pos: int, policy_text: str) -> None:
            try:
                task_description = self.task_description
                query = (
                    f"Task description:\n{task_description}\n\n"
                    f"Policy:\n{policy_text}\n\n"
                    f"Sample:\n{sample}\n\n"
                )
                async with self._llm_semaphore:
                    response = await llm.respond(
                        query=query,
                        llm_priority=self.predict_llmc,
                        instructions=POLICY_PREDICT_INSTRUCTIONS,
                        response_format=Answer,
                        temperature=self.predict_temperature,
                    )

                await token_counter.append(
                    provider=response.provider_model.provider,
                    model=response.provider_model.model,
                    value=response.total_tokens,
                    caller="Policy Induction._predict_single",
                )
                if response.response is None:
                    raise LLMError("No response from LLM")

                if response.average_confidence is not None:
                    logger.debug(
                        f"Confidence: {response.average_confidence} " f"pos={pos}"
                    )

                txt = str(response.response.answer).strip().upper()
                txt = txt.strip().strip('".,;:')

                if txt in {"YES", "NO"}:
                    results[pos] = 1.0 if txt == "YES" else 0.0
                else:
                    if "YES" in txt:
                        results[pos] = 1.0
                    elif "NO" in txt:
                        results[pos] = 0.0
                    else:
                        logger.warning(
                            "Unexpected model output at pos " "%s: %r", pos, response
                        )
                        results[pos] = 0.0

            except asyncio.CancelledError:
                pass
            except Exception:
                logger.warning(
                    "Error in PolicyInduction worker prediction", exc_info=True
                )
                results[pos] = 0.0
            finally:
                completion_queue.put_nowait(None)

        tasks_to_run: list[tuple[int, str]] = []
        for pos, policy_text in enumerate(policies.fillna("").astype(str).values):
            if policy_text.strip():
                tasks_to_run.append((pos, policy_text))

        in_flight = 0
        try:
            async with asyncio.TaskGroup() as tg:
                for _ in range(min(self.llm_semaphore_limit, len(tasks_to_run))):
                    pos, policy_text = tasks_to_run.pop(0)
                    tg.create_task(worker(pos, policy_text))
                    in_flight += 1

                while in_flight > 0:
                    await completion_queue.get()
                    in_flight -= 1

                    if tasks_to_run:
                        pos, policy_text = tasks_to_run.pop(0)
                        tg.create_task(worker(pos, policy_text))
                        in_flight += 1
        finally:
            final_pred = self._lr_predict(results)
            return sample_index, results, final_pred

    async def predict(
        self, samples: pd.DataFrame
    ) -> AsyncGenerator[Tuple[Any, NDArray, Literal["YES", "NO"], TokenCounter], None]:
        """Predict labels for samples.

        Args:
            samples: Samples to predict.

        Returns:
            Generator of predictions[sample_index, question, answer, token_counter]

        Raises:
            ValueError: If samples is empty or does not have the correct column.
        """
        self._check_memory()

        queue: asyncio.Queue[
            Literal["DONE"] | Tuple[Any, NDArray, Literal["YES", "NO"]]
        ] = asyncio.Queue()

        token_counter = TokenCounter()

        async def worker(sample_index: Any, sample: str) -> None:
            try:
                rec = await self._predict_single(
                    sample_index=sample_index,
                    sample=sample,
                    token_counter=token_counter,
                )
                await queue.put(rec)
            finally:
                await queue.put("DONE")

        tasks: List[asyncio.Task[None]] = []
        try:
            for sample_index, row in samples.iterrows():
                sample_str = "\n".join([f"{col}: {val}" for col, val in row.items()])
                tasks.append(asyncio.create_task(worker(sample_index, sample_str)))

            remaining = len(tasks)
            while remaining > 0:
                item = await queue.get()
                if item == "DONE":
                    remaining -= 1
                    continue
                else:
                    yield item + (token_counter,)
            return
        except asyncio.CancelledError:
            pass
        finally:
            for task in tasks:
                if not task.done():
                    task.cancel()

    async def add_policy(self, policy: str) -> Literal[True]:
        """Add a new policy to the PolicyInduction.

        Args:
            policy: The policy to add.

        Raises:
            ValueError: If policy not str.
        """
        if not isinstance(policy, str):
            raise ValueError(f"Policy should be str, get {type(policy)} instead")

        self._policy_memory.at[len(self._policy_memory), "policy"] = policy

        logger.info("Scoring Policies")
        await self._score_policies()

        logger.info("Setting policy weight")
        self._fit_weights()
        return True

    def save(
        self,
        dir_path: str | PathLike[str] | None = None,
        for_production: bool = False,
    ) -> None:
        """Persist the model state to disk.

        This saves a manifest JSON plus parquet artifacts in a target directory.
        Layout:
        - policy_induction.json      (manifest/config/state)
        - data.parquet               (X + y; only when not for_production)
        - policies.parquet           (policy_id, policy)
        - policy_predictions.parquet (long-form: sample_index, policy_id, pred;
                                        only when not for_production)
        - lr.joblib                  (trained LogisticRegression, if available)
        """
        base = Path(dir_path) if dir_path is not None else self.save_path

        if base.is_file():
            raise ValueError("Please provide a directory, not a file.")
        base.mkdir(parents=True, exist_ok=True)

        # Save training data (only in non-production mode)
        if not for_production:
            if sum([self._X is not None, self._y is not None]) == 1:
                raise ValueError("Model state is corrupted. Either X or y is missing.")
            if self._X is not None:
                df = deepcopy(self._X)
                df["y"] = self._y
                df.to_parquet(base / "data.parquet")  # type: ignore

        # Save policies (policy text only)
        pm = self._policy_memory.copy()
        policies_df = (
            pm.drop(columns=["predictions"], errors="ignore")
            .reset_index()
            .rename(columns={"index": "policy_id"})
        )
        policies_df["policy_id"] = policies_df["policy_id"].astype(str)
        policies_df.to_parquet(base / "policies.parquet")

        # Save policy predictions on training set in long form (only non-production)
        if not for_production and "predictions" in pm.columns:
            pred_rows: list[dict] = []
            for pid, preds in pm["predictions"].items():
                if isinstance(preds, pd.Series):
                    for sidx, val in preds.items():
                        pred_rows.append(
                            {
                                "sample_index": sidx,
                                "policy_id": str(pid),
                                "pred": None if pd.isna(val) else str(val),
                            }
                        )
            if pred_rows:
                pd.DataFrame(pred_rows).to_parquet(base / "policy_predictions.parquet")

        # Save trained logistic regression (if available)
        model_file = None
        if getattr(self, "_lr", None) is not None:
            model_file = "lr.joblib"
            joblib_dump(self._lr, base / model_file)  # type: ignore

        # Build and save manifest
        manifest = {
            "name": self.name,
            "gen_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.gen_llmc
            ],
            "predict_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.predict_llmc
            ],
            "gen_temperature": self.gen_temperature,
            "predict_temperature": self.predict_temperature,
            "llm_semaphore_limit": self.llm_semaphore_limit,
            "max_policy_length": self.max_policy_length,
            "class_ratio": list(self.class_ratio),
            "max_samples_as_context": self.max_samples_as_context,
            "p_predict_update_interval": self.p_predict_update_interval,
            "token_counter": None if for_production else self._token_counter.to_dict(),
            "save_path": str(self.save_path) if not for_production else None,
            "task_description": self._task_description,
            "policy_gen_instructions_template": (
                self._pgen_instructions_template if not for_production else None
            ),
            "random_state": self.random_state,
            "threshold": self._threshold,
            "validation_result": self._validation_result,
            "feature_order": (
                getattr(self, "_feature_order_", None).tolist()  # type: ignore
                if hasattr(self, "_feature_order_") and self._feature_order_ is not None
                else None
            ),
            "n_features": getattr(self, "_n_features_", None),
            "model_file": model_file,
            "config": asdict(self.config),
            "version": 1,
        }
        with (base / "policy_induction.json").open("wb") as f:
            f.write(orjson.dumps(manifest, option=orjson.OPT_SERIALIZE_NUMPY))

    @classmethod
    def _load(cls, dir_path: str | PathLike[str]) -> "PolicyInduction":
        """Load a PolicyInduction instance previously saved with `save`."""
        from joblib import load as joblib_load
        import orjson

        base = Path(dir_path)
        if not base.is_dir():
            raise ValueError("Please provide a directory, not a file.")

        manifest_path = base / "policy_induction.json"
        if not manifest_path.exists():
            raise FileNotFoundError(
                "'policy_induction.json' " f"not found in directory: {base}"
            )

        with manifest_path.open("rb") as f:
            manifest = orjson.loads(f.read())

        # Recreate instance using manifest configuration
        inst = cls(
            gen_llmc=manifest["gen_llmc"],
            predict_llmc=manifest["predict_llmc"],
            config=manifest.get("config"),
            gen_temperature=manifest["gen_temperature"],
            predict_temperature=manifest["predict_temperature"],
            llm_semaphore_limit=manifest["llm_semaphore_limit"],
            max_policy_length=manifest["max_policy_length"],
            class_ratio=tuple(manifest["class_ratio"]),
            max_samples_as_context=manifest["max_samples_as_context"],
            p_predict_update_interval=manifest["p_predict_update_interval"],
            random_state=manifest["random_state"],
            save_path=str(base.parent),
            name=manifest["name"],
        )

        # Restore runtime attributes
        inst._task_description = manifest.get("task_description")
        inst._pgen_instructions_template = manifest.get(
            "policy_gen_instructions_template"
        )
        tk_dict = manifest.get("token_counter")
        if tk_dict:
            inst._token_counter = TokenCounter.from_dict(tk_dict)
        inst._threshold = manifest.get("threshold", 0.0)
        inst._validation_result = manifest.get("validation_result")

        # Restore semaphore
        inst._llm_semaphore = asyncio.Semaphore(inst.llm_semaphore_limit)

        # Restore feature order metadata
        feature_order = manifest.get("feature_order")
        if feature_order is not None:
            inst._feature_order_ = np.array(feature_order, dtype=str)
            inst._n_features_ = int(
                manifest.get("n_features", len(inst._feature_order_))
            )
            inst._policy_pos_ = {name: i for i, name in enumerate(inst._feature_order_)}

        # Load training data (if present)
        data_path = base / "data.parquet"
        if data_path.exists():
            data_df = pd.read_parquet(data_path)  # type: ignore
            if "y" in data_df.columns:
                inst._y = data_df["y"].to_numpy(dtype=np.str_)  # type: ignore
                inst._X = data_df.drop(columns=["y"])  # type: ignore

        # Load policies (required)
        policies_path = base / "policies.parquet"
        if not policies_path.exists():
            raise FileNotFoundError(
                f"'policies.parquet' not found in directory: {base}"
            )
        p_df = pd.read_parquet(policies_path)  # type: ignore
        if "policy_id" not in p_df.columns or "policy" not in p_df.columns:
            raise ValueError(
                "policies.parquet must contain columns: ['policy_id', 'policy']"
            )
        p_df["policy_id"] = p_df["policy_id"].astype(str)
        p_df = p_df.set_index("policy_id")

        # Rebuild _policy_memory skeleton; fill predictions later if available
        inst._policy_memory = pd.DataFrame(
            {
                "policy": p_df["policy"],
                "predictions": pd.Series(
                    [None] * len(p_df), index=p_df.index, dtype=object
                ),
            }
        )

        # Load cached policy predictions for training set
        preds_path = base / "policy_predictions.parquet"
        if preds_path.exists() and inst._X is not None:
            pp = pd.read_parquet(preds_path)  # type: ignore
            required_cols = {"sample_index", "policy_id", "pred"}
            if not required_cols.issubset(pp.columns):
                raise ValueError(
                    "policy_predictions.parquet must contain columns: "
                    "'sample_index', 'policy_id', 'pred'"
                )
            pp["policy_id"] = pp["policy_id"].astype(str)
            for pid, g in pp.groupby("policy_id"):
                s = pd.Series(
                    g["pred"].values, index=g["sample_index"].values, dtype="object"
                )
                s = s.reindex(inst._X.index)  # align to training index
                if pid in inst._policy_memory.index:
                    inst._policy_memory.at[pid, "predictions"] = s

        # Load trained LR model (if provided)
        model_file = manifest.get("model_file")
        if model_file:
            model_path = base / model_file
            if model_path.exists():
                inst._lr = joblib_load(model_path)

        return inst

    @classmethod
    def load(cls, dir_path: str | PathLike[str]) -> "PolicyInduction":
        """Public loader that wraps `_load` with friendlier errors."""
        try:
            return cls._load(dir_path)
        except KeyError as e:
            raise ValueError(
                f"Failed to load PolicyInduction. Manifest likely corrupted: {e}"
            )
        except Exception as e:
            raise e

    def __repr__(self) -> str:
        return f"PolicyInduction(name={self.name})"

    def __str__(self) -> str:
        return f"PolicyInduction(name={self.name})"
