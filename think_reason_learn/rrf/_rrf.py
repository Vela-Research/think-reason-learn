"""RRF.

An interpretable ensemble framework for binary classification based on YES/NO questions
generated by LLMs.
"""

from __future__ import annotations

from typing import List, Self, Tuple, cast, Generator, Any, Literal, Sequence
from typing import Iterable, AsyncGenerator, Dict
import asyncio
import logging
from copy import deepcopy
from enum import StrEnum
from importlib.util import find_spec
from uuid import uuid4
from pathlib import Path
from os import PathLike
import os

import numpy as np
import numpy.typing as npt
import pandas as pd
from pydantic import BaseModel, Field
import orjson

from think_reason_learn.core.llms import LLMChoice, llm, TokenCounter
from think_reason_learn.core.exceptions import CorruptionError, DataError, LLMError
from ._prompts import num_questions_tag, CUMULATIVE_MEMORY_INSTRUCTIONS
from ._prompts import QUESTION_GEN_INSTRUCTIONS, QUESTION_ANSWER_INSTRUCTIONS
from ._types import EmbeddingModel, AnsSimilarityFunc
import re


logger = logging.getLogger(__name__)


class Questions(BaseModel):
    questions: List[str] = Field(..., description="The list of generated questions.")
    cumulative_memory: str = Field(..., description=CUMULATIVE_MEMORY_INSTRUCTIONS)


class Answer(BaseModel):
    answer: Literal["YES", "NO"]


class QuestionExclusion(StrEnum):
    SEMANTICS = (
        "semantics",
        "Excluded because of semantic overlap with other questions",
    )
    PREDICTION_SIMILARITY = (
        "prediction_similarity",
        "Excluded due to high prediction similarity with other questions",
    )
    EXPERT = ("expert", "Excluded manually by expert")

    def __new__(cls, value: str, description: str) -> QuestionExclusion:
        obj = str.__new__(cls, value)
        obj._value_ = value
        obj.description = description  # type: ignore
        return obj


class RRF:
    """Interpretable ensemble binary classifier.

    Args:
        qgen_llmc: LLMs to use for question generation, in priority order.
        qanswer_llmc: LLMs to use for answering questions, in priority order.
            If None, use qgen_llmc.
        qgen_temperature: Sampling temperature for question generation.
        qanswer_temperature: Sampling temperature for answering questions.
        llm_semaphore_limit: Max concurrent LLM calls.
        answer_similarity_func: Function to use for answer similarity.
        max_generated_questions: Maximum number of questions to generate. Max 1000
        max_samples_as_context: Number of samples used as context in a round
            of question generation. max 100 Max 100
        class_ratio: Ratio of YES to NO samples to use as context in a round
            of question generation.
        q_answer_update_interval: Logging interval of question answering.
        save_path: Directory to save checkpoints/models.
        name: Name of the forest instance.
        random_state: Random seed.
    """

    def __init__(
        self,
        qgen_llmc: List[LLMChoice],
        qanswer_llmc: List[LLMChoice] | None = None,
        qgen_temperature: float = 0.0,
        qanswer_temperature: float = 0.0,
        llm_semaphore_limit: int = 3,
        answer_similarity_func: AnsSimilarityFunc = "hamming",
        max_generated_questions: int = 100,
        max_samples_as_context: int = 30,
        class_ratio: Tuple[float, float] = (1.0, 1.0),
        q_answer_update_interval: int = 10,
        save_path: str | PathLike[str] | None = None,
        name: str | None = None,
        random_state: int = 42,
    ):
        locals_dict = deepcopy(locals())
        del locals_dict["self"]
        self._verify_input_data(**locals_dict)

        self.qgen_llmc = qgen_llmc
        self.qanswer_llmc = qanswer_llmc or qgen_llmc
        self.qgen_temperature = qgen_temperature
        self.qanswer_temperature = qanswer_temperature
        self._llm_semaphore_limit = llm_semaphore_limit
        self.answer_similarity_func = answer_similarity_func
        self.max_generated_questions = max_generated_questions
        self.max_samples_as_context = max_samples_as_context
        self.class_ratio = class_ratio
        self.name: str = self._get_name(name)
        self.save_path: Path = self._set_save_path(save_path)
        self.q_answer_update_interval = q_answer_update_interval
        self.random_state = random_state

        self._token_counter: TokenCounter = TokenCounter()

        self._llm_semaphore = asyncio.Semaphore(llm_semaphore_limit)
        self._qgen_instructions_template: str | None = None
        self._classes: List[str] | None = None
        self._X: pd.DataFrame | None = None
        self._y: npt.NDArray[np.str_] | None = None
        self._task_description: str | None = None
        self._questions: pd.DataFrame = self._get_initial_questions_df()
        self._answers: pd.DataFrame = self._create_answers_df(index=None)
        self._last_emb_model: EmbeddingModel | None = None

    @property
    def llm_semaphore_limit(self) -> int:
        return self._llm_semaphore_limit

    @llm_semaphore_limit.setter
    def llm_semaphore_limit(self, value: int) -> None:
        self._llm_semaphore_limit = value
        self._llm_semaphore = asyncio.Semaphore(value)

    def _verify_input_data(self, **kwargs: Any) -> None:
        val = kwargs["max_generated_questions"]
        if not (0 < val < 1000):
            raise ValueError("max_generated_questions must be > 0 and < 1000")
        val = kwargs["max_samples_as_context"]
        if not (0 < val <= 100):
            raise ValueError("max_samples_as_context must be > 0 and <= 100")
        val = kwargs["class_ratio"]
        if not (len(val) == 2 and all(cr > 0 for cr in val)):
            raise ValueError("class_ratio must be a tuple of two elements each > 0")
        val = kwargs["q_answer_update_interval"]
        if not (val > 0):
            raise ValueError("q_answer_update_interval must be > 0")
        val = kwargs["llm_semaphore_limit"]
        if not (val > 0):
            raise ValueError("llm_semaphore_limit must be > 0")
        val = kwargs["save_path"]
        if not (val is None or isinstance(val, (str, Path))):
            raise ValueError("save_path must be None, a string, or a Path")
        val = kwargs["name"]
        if not (val is None or isinstance(val, str)):
            raise ValueError("name must be None or a string")
        val = kwargs["qgen_temperature"]
        if not (0 <= val <= 2):
            raise ValueError("qgen_temperature must be >= 0 and <= 2")
        val = kwargs["qanswer_temperature"]
        if not (0 <= val <= 2):
            raise ValueError("qanswer_temperature must be >= 0 and <= 2")
        val = kwargs["answer_similarity_func"]
        if val not in ["hamming", "jaccard"]:
            raise ValueError("answer_similarity_func must be 'hamming' or 'jaccard'")

    def _get_name(self, name: str | None) -> str:
        if name is None:
            name = str(uuid4()).replace("-", "_")
            logger.debug(f"No name provided. Assigned name: {name}")

        if not re.match(r"^[a-zA-Z0-9_]+$", name):
            raise ValueError("Name must be only alphanumeric and underscores")
        return name

    def _set_save_path(self, save_path: str | PathLike[str] | None) -> Path:
        if save_path is None:
            return (Path(os.getcwd()) / "rrfs" / self.name).resolve()
        else:
            save_path = Path(save_path).resolve()
            if save_path.is_file():
                raise ValueError("Please provide a directory, not a file.")
            return save_path

    @property
    def token_usage(self) -> TokenCounter:
        """Get the token counter for the RRF."""
        return self._token_counter

    def _get_initial_questions_df(self) -> pd.DataFrame:
        """Initialize the questions dataframe."""
        return pd.DataFrame(
            {
                "question": pd.Series([], dtype=str),
                "embedding": pd.Series([], dtype=object),
                "exclusion": pd.Series([], dtype=str),
                "precision": pd.Series([], dtype=float),
                "recall": pd.Series([], dtype=float),
                "f1_score": pd.Series([], dtype=float),
                "accuracy": pd.Series([], dtype=float),
            },
            index=pd.Index([], dtype=str, name="id"),
        )

    def get_questions(self) -> pd.DataFrame:
        """Get generated questions dataframe.

        Returns:
            DataFrame containing:
                - question: Generated question text
                - embedding: Question embedding vectors
                - exclusion: Question exclusion principle
                - precision: Precision for each question
                - recall: Recall for each question
                - f1_score: F1 score for each question
                - accuracy: Accuracy for each question
        """
        return self._questions

    def _create_answers_df(self, index: pd.Index | None = None) -> pd.DataFrame:
        """Initialize the answers dataframe."""
        # A dataframe with columns as the answered questions ids (questions not excluded
        # during semantic filtering) and index as the samples indices from self._X
        return pd.DataFrame({}, index=index)

    def get_answers(self) -> pd.DataFrame:
        """Get answers dataframe.

        Returns:
            DataFrame containing:
                - Columns as the questions ids (not excluded during semantic filtering).
                - Index as the samples indices from self._X.
        """
        return self._answers

    @property
    def question_gen_instructions_template(self) -> str | None:
        """Get the question generation instructions template."""
        return self._qgen_instructions_template

    @property
    def task_description(self) -> str | None:
        """Get the task description."""
        return self._task_description

    async def set_tasks(
        self,
        instructions_template: str | None = None,
        task_description: str | None = None,
    ) -> str:
        """Initialize question generation instructions template.

        This sets the task description for the RRF.
        Either sets a custom template or generates one from task description using LLM.
        For most users, LLM generation is recommended over custom templates.

        Args:
            instructions_template: Custom template to use. Must contain
                '<number_of_questions>' tag. If None, generates template
                from task_description using LLM.
            task_description: Description of classification task to help LLM generate
                the template.

        Returns:
            The question generation instructions template.

        Raises:
            ValueError: If template missing required tag or generation fails.
            AssertionError: If both parameters are None.
        """
        assert (
            instructions_template is not None or task_description is not None
        ), "Either instructions_template or task_description must be provided"

        if instructions_template:
            if num_questions_tag not in instructions_template:
                raise ValueError(
                    f"instructions_template must contain the tag '{num_questions_tag}' "
                    "This tag will be replaced with the number of questions to generate"
                )
            else:
                self._qgen_instructions_template = instructions_template
                return instructions_template

        async with self._llm_semaphore:
            response = await llm.respond(
                query=f"Generate YES/NO questions for:\n{task_description}",
                llm_priority=self.qgen_llmc,
                response_format=str,
                instructions=QUESTION_GEN_INSTRUCTIONS,
                temperature=self.qgen_temperature,
            )
        await self._token_counter.append(
            provider=response.provider_model.provider,
            model=response.provider_model.model,
            value=response.total_tokens,
            caller="RRF._set_tasks",
        )
        if not response.response:
            raise ValueError(
                "Failed to generate question generation instructions. "
                "Try refining the task description or change the models "
                "for generating question generation instructions (self.qgen_llmc)."
            )
        elif num_questions_tag not in response.response:
            raise ValueError(
                "Failed to generate a valid question generation "
                "instructions template. Please try again."
            )

        if response.average_confidence is not None:
            logger.info(
                "Generated question generation instructions with "
                f"confidence {response.average_confidence}"
            )
        else:
            logger.info(
                "Generated question generation instructions. Could not track "
                "confidence of instructions."
            )

        self._task_description = task_description
        self._qgen_instructions_template = response.response
        return response.response

    def _get_question_gen_instructions(self, num_questions: int) -> str:
        if not self._qgen_instructions_template:
            raise ValueError(
                "Question generation instructions template is not set. "
                "Set the template using `set_tasks`."
            )

        return self._qgen_instructions_template.replace(
            num_questions_tag, str(num_questions)
        )

    def _sample(self, n: int) -> Generator[pd.DataFrame, None, None]:
        if self._X is None or self._y is None:
            raise ValueError("X and y must be set")

        rng = np.random.default_rng(self.random_state)
        yes_idx = rng.permutation(np.where(self._y == "YES")[0])
        no_idx = rng.permutation(np.where(self._y == "NO")[0])

        p_yes = self.class_ratio[0] / sum(self.class_ratio)
        want_yes = int(round(n * p_yes))
        want_no = n - want_yes

        taken_y = 0
        taken_n = 0
        len_yes = len(yes_idx)
        len_no = len(no_idx)

        while taken_y < len_yes or taken_n < len_no:
            take_y = min(want_yes, len_yes - taken_y)
            take_n = min(want_no, len_no - taken_n)

            if take_y == 0 and take_n == 0:
                break

            # Fill from remaining if one is short
            if take_y + take_n < n:
                rem = n - take_y - take_n
                if len_yes - taken_y > 0:
                    extra = min(rem, len_yes - taken_y - take_y)
                    take_y += extra
                    rem -= extra
                if rem > 0 and len_no - taken_n > 0:
                    take_n += min(rem, len_no - taken_n - take_n)

            batch = np.concatenate(
                [
                    yes_idx[taken_y : taken_y + take_y],
                    no_idx[taken_n : taken_n + take_n],
                ]
            )
            if batch.size == 0:
                break

            rng.shuffle(batch)
            batch_df = self._X.iloc[batch].copy()  # type: ignore
            batch_df["y"] = self._y[batch]
            yield batch_df

            taken_y += take_y
            taken_n += take_n

    async def _generate_questions(self) -> None:
        if self._X is None:
            raise ValueError("X must be set")

        num_llm_calls = max(
            1, int(np.ceil(self._X.shape[0] / self.max_samples_as_context))
        )
        num_questions_per_call = self.max_generated_questions // num_llm_calls
        instructions = self._get_question_gen_instructions(num_questions_per_call)
        cumulative_memory = "No cumulative memory yet. This is the first generation."
        all_questions: List[str] = []

        for sample_df in self._sample(self.max_samples_as_context):
            samples_str = ""
            for each_sample in sample_df.to_dict(orient="records"):  # type: ignore
                sample_str = "\n".join(
                    [f"{col}: {val}" for col, val in each_sample.items()]  # type: ignore
                )
                samples_str += f"\n{sample_str};"

            query = f"SAMPLES:\n{samples_str}\n\nCUMULATIVE MEMORY: {cumulative_memory}"

            async with self._llm_semaphore:
                response = await llm.respond(
                    query=query,
                    llm_priority=self.qgen_llmc,
                    response_format=Questions,
                    instructions=instructions,
                    temperature=self.qgen_temperature,
                )

            await self._token_counter.append(
                provider=response.provider_model.provider,
                model=response.provider_model.model,
                value=response.total_tokens,
                caller="RRF._generate_questions",
            )
            questions = response.response
            if questions is None:
                raise LLMError(
                    "Could not generate questions. Please try "
                    "again or with a different llm."
                    f"\nQuery: {query[:200]}..."
                    f"\n\nCumulative memory: {cumulative_memory[:200]}..."
                    f"\n\nInstructions: {instructions[:200]}..."
                )
            all_questions.extend(questions.questions)
            cumulative_memory = questions.cumulative_memory
            logger.info(f"Cumulative memory: {cumulative_memory}")
            logger.info(f"Generated questions: {len(questions.questions)}")

        qlen = len(all_questions)
        max_questions_id_len = 3  # 3 digits for 999 max questions
        index = [str(i).zfill(max_questions_id_len) for i in range(qlen)]

        self._questions = pd.DataFrame(
            {
                "question": all_questions,
                "embedding": [None] * qlen,
                "exclusion": [None] * qlen,
                "precision": [None] * qlen,
                "recall": [None] * qlen,
                "f1_score": [None] * qlen,
                "accuracy": [None] * qlen,
            },
            index=index,  # type: ignore
        )

    def _hash_bag_of_words_emb_single(self, text: str) -> np.ndarray:
        dim = 256
        token_pattern = re.compile(r"\b\w+\b")
        vec = np.zeros(dim, dtype=np.float32)
        for token in token_pattern.findall(text.lower()):
            h = hash(token)
            idx = (h % dim) if h >= 0 else ((-h) % dim)
            sign = 1.0 if (h & 1) == 0 else -1.0
            vec[idx] += sign
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec /= norm
        return vec

    def _hash_bag_of_words_emb(self, text: List[str]) -> np.ndarray:
        return np.vstack([self._hash_bag_of_words_emb_single(q) for q in text])

    async def _sentence_transformer_emb(
        self, text: List[str], emb_model: EmbeddingModel
    ) -> np.ndarray:
        """Embed text using the sentence transformer."""
        if find_spec("sentence_transformers") is None:
            raise ImportError(
                f"Unknown embedding model: {emb_model} "
                "If a sentence transformer model, install sentence_transformers "
                "with `pip install sentence-transformers`."
            )

        from sentence_transformers import SentenceTransformer  # type: ignore

        encode_fn = cast(Any, SentenceTransformer(emb_model).encode_async)
        embeddings_np = await encode_fn(
            text, convert_to_numpy=True, normalize_embeddings=True
        )
        embeddings_np = cast(np.ndarray, embeddings_np)
        return embeddings_np.astype(np.float32, copy=False)

    async def _set_questions_semantics(self, emb_model: EmbeddingModel) -> None:
        """Embed questions using the embedding model."""
        if self._questions.empty:
            return

        if self._last_emb_model != emb_model:
            self._last_emb_model = emb_model
            self._questions["embedding"] = None

        qdf = self._questions
        to_embed_positions: List[int] = []
        texts_to_embed: List[str] = []
        for pos, (_row_id, row) in enumerate(qdf.iterrows()):
            emb_any = cast(Any, row["embedding"])
            needs_emb: bool
            if isinstance(emb_any, np.ndarray):
                needs_emb = bool(emb_any.size == 0)
            else:
                needs_emb = True
            if needs_emb:
                to_embed_positions.append(pos)
                row_label = cast(str, qdf.index[pos])
                question_text = cast(str, qdf.at[row_label, "question"])  # type: ignore
                texts_to_embed.append(question_text)

        if not to_embed_positions:
            return

        embeddings: np.ndarray
        if emb_model == "hashed_bag_of_words":
            embeddings = self._hash_bag_of_words_emb(texts_to_embed)
        else:
            embeddings = await self._sentence_transformer_emb(texts_to_embed, emb_model)

        for i, pos in enumerate(to_embed_positions):
            row_label = cast(str, qdf.index[pos])
            self._questions.at[row_label, "embedding"] = embeddings[i]

    async def filter_questions_on_semantics(
        self,
        threshold: float | None,
        emb_model: EmbeddingModel,
    ) -> None:
        """Filter questions on semantics.

        If two questions have a semantic similarity greater than or
        equal to the threshold, the question with the lower f1 score is excluded.

        Args:
            threshold: Threshold for semantic filtering. If None, no filtering is
            done based on semantic similarity.
            emb_model: Embedding model to use for semantic filtering.

        Raises:
            AssertionError: If threshold is not between > 0 and <= 1.
        """
        if threshold is not None:
            assert 0 < threshold <= 1, "Threshold must be between > 0 and <= 1"

        if self._questions.empty:
            return

        if threshold is None:
            # Do not clear if semantics aren't set yet except threshold is None
            self._questions.loc[
                self._questions["exclusion"] == QuestionExclusion.SEMANTICS.value,
                "exclusion",
            ] = None
            return

        await self._set_questions_semantics(emb_model)
        self._questions.loc[
            self._questions["exclusion"] == QuestionExclusion.SEMANTICS.value,
            "exclusion",
        ] = None

        qdf = self._questions
        embeddings_list: List[npt.NDArray[np.float32]] = []
        indices: List[int] = []
        for idx, (_row_idx, row) in enumerate(qdf.iterrows()):
            emb_any = cast(Any, row["embedding"])
            if isinstance(emb_any, np.ndarray) and emb_any.size > 0:
                embeddings_list.append(emb_any)  # type: ignore
                indices.append(idx)

        if len(embeddings_list) <= 1:
            return

        emb_matrix = np.vstack(embeddings_list)

        # Normalize (defensive in case upstream did not)
        norms = np.linalg.norm(emb_matrix, axis=1, keepdims=True)
        norms[norms == 0] = 1.0
        emb_matrix = emb_matrix / norms

        # F1-aware deduplication: within each similar group, keep highest-F1
        def _safe_f1(qid: str) -> float:
            if "f1_score" not in self._questions.columns:
                return 0.0
            val = cast(Any, self._questions.at[qid, "f1_score"])  # type: ignore
            if isinstance(val, (int, float)):
                return float(val)
            if isinstance(val, np.generic):
                try:
                    return float(val.item())  # type: ignore[no-any-return]
                except Exception:
                    return 0.0
            return 0.0

        nrows = emb_matrix.shape[0]
        keep_flags = np.ones(nrows, dtype=bool)
        for i in range(nrows):
            if not keep_flags[i]:
                continue
            group: List[int] = [i]
            if i + 1 < nrows:
                sims = emb_matrix[i + 1 :] @ emb_matrix[i]
                for off, is_dup in enumerate(sims >= threshold):
                    k = i + 1 + off
                    if bool(is_dup) and keep_flags[k]:
                        group.append(k)

            if len(group) > 1:
                # Pick best by F1 score
                def f1_for_row(r: int) -> float:
                    pos = indices[r]
                    qid = cast(str, self._questions.index[pos])
                    return _safe_f1(qid)

                best = max(group, key=f1_for_row)
                for r in group:
                    keep_flags[r] = False
                keep_flags[best] = True

        # Map back to original dataframe indices
        excluded_indices = [indices[i] for i, keep in enumerate(keep_flags) if not keep]
        if excluded_indices:
            qdf.loc[qdf.index[excluded_indices], "exclusion"] = (
                QuestionExclusion.SEMANTICS.value
            )

    async def _answer_single_question(
        self,
        sample_index: Any,
        sample: str,
        question_id: str,
        question: str,
        token_counter: TokenCounter,
    ) -> Tuple[Any, str, Answer] | None:
        """Returns sample_index, question_id, answer."""
        try:
            async with self._llm_semaphore:
                response = await llm.respond(
                    llm_priority=self.qanswer_llmc,
                    query=f"Query: {question}\n\nSample: {sample}",
                    instructions=QUESTION_ANSWER_INSTRUCTIONS,
                    response_format=Answer,
                    temperature=self.qanswer_temperature,
                )
            await token_counter.append(
                provider=response.provider_model.provider,
                model=response.provider_model.model,
                value=response.total_tokens,
                caller="RRF._answer_single_question",
            )
            if response.response is None:
                raise LLMError("No response from LLM")

            if response.average_confidence is not None:
                logger.debug(
                    f"Confidence: {response.average_confidence} "
                    f"Answered question: '{question}' for sample: {sample}"
                )
            else:
                logger.debug(
                    "Could not track confidence of answer. "
                    f"Answered question: '{question}' for sample: {sample}"
                )
            return sample_index, question_id, response.response

        except Exception as _:
            logger.warning(
                f"Error answering question: '{question}' for sample: {sample}",
                exc_info=True,
            )

    def _update_answers_df_columns(self) -> None:
        """Ensure all non excluded questions are in the answers dataframe."""
        not_excluded_df = cast(
            pd.DataFrame,
            self._questions[self._questions["exclusion"].isna()],  # type: ignore
        )
        if not_excluded_df.shape[0] == 0:
            return
        cat_dtype: Any = pd.CategoricalDtype(categories=["YES", "NO"])
        target_cols: List[str] = cast(List[str], list(not_excluded_df.index))

        missing = [q for q in target_cols if q not in self._answers.columns]
        for q in missing:
            self._answers[q] = pd.Series(
                pd.Categorical([pd.NA] * len(self._answers.index), dtype=cat_dtype),
                index=self._answers.index,
                dtype=cat_dtype,
            )

        coerce_map: Dict[str, Any] = {
            q: cat_dtype for q in target_cols if q in self._answers.columns
        }
        if coerce_map:
            answers_any: Any = self._answers
            self._answers = cast(pd.DataFrame, answers_any.astype(coerce_map))

    async def _answer_questions(self) -> None:
        if self._X is None or self._y is None:
            raise ValueError("X and y must be set")

        self._update_answers_df_columns()

        mask = self._answers.isna().stack()
        not_answered = cast(Iterable[Tuple[int, str]], mask[mask].index)  # type: ignore
        num_not_answered = not_answered.size  # type: ignore

        # Buffered updates: per-question updates applied in batches
        answers_buffer: Dict[str, Dict[Any, str]] = {}

        completion_queue: asyncio.Queue[None] = asyncio.Queue()

        async def worker(sample_index: Any, question_id: str) -> None:
            try:
                qdf = self._questions
                xdf = self._X
                question = cast(str, qdf.at[question_id, "question"])  # type: ignore
                sample = cast(str, xdf.iloc[sample_index])  # type: ignore
                sample_str = "\n".join(
                    [f"{col}: {val}" for col, val in sample.items()]  # type: ignore
                )
                result = await self._answer_single_question(
                    sample_index=sample_index,
                    sample=sample_str,
                    question_id=question_id,
                    question=question,
                    token_counter=self._token_counter,
                )
                if result is not None:
                    sidx, qid, answer = result
                    bucket = answers_buffer.get(qid)
                    if bucket is None:
                        bucket = {}
                        answers_buffer[qid] = bucket
                    bucket[sidx] = answer.answer.upper()
            except asyncio.CancelledError:
                pass
            except Exception:
                logger.warning("Error in RRF worker answering question", exc_info=True)
            finally:
                completion_queue.put_nowait(None)

        not_answered_iter = iter(not_answered)
        in_flight = 0
        completions = 0

        try:
            async with asyncio.TaskGroup() as tg:
                for _ in range(self.llm_semaphore_limit):
                    try:
                        sidx, qid = next(not_answered_iter)
                    except StopIteration:
                        break
                    tg.create_task(worker(sidx, qid))
                    in_flight += 1

                while in_flight > 0:
                    await completion_queue.get()
                    in_flight -= 1
                    completions += 1
                    if completions % self.q_answer_update_interval == 0:
                        logger.info(
                            f"Answered {completions} questions out "
                            f"of {num_not_answered}"
                        )
                    try:
                        sidx, qid = next(not_answered_iter)
                    except StopIteration:
                        continue
                    tg.create_task(worker(sidx, qid))
                    in_flight += 1
        finally:
            ansdf = self._answers
            for qid, mapping in answers_buffer.items():
                if not mapping:
                    continue
                # Ensure destination column exists with proper categorical dtype
                if qid not in ansdf.columns:
                    self._update_answers_df_columns()
                series_update = pd.Series(mapping, dtype=object)
                index_update = cast(pd.Index, series_update.index)
                ansdf.loc[index_update, qid] = series_update

    def _set_questions_metrics(self) -> None:
        if self._y is None:
            return

        y_series = pd.Series(self._y, index=self._answers.index)

        true_yes = set(y_series.index[y_series == "YES"])  # type: ignore
        true_no = set(y_series.index[y_series == "NO"])  # type: ignore

        for qid in cast(List[str], self._questions.index):
            if qid not in self._answers.columns:
                continue

            col = cast(pd.Series, self._answers[qid])
            pred_yes = set(col.index[col == "YES"])  # type: ignore
            pred_no = set(col.index[col == "NO"])  # type: ignore

            tp = len(pred_yes & true_yes)
            tn = len(pred_no & true_no)
            fp = len(pred_yes & true_no)
            fn = len(true_yes - pred_yes)

            precision = tp / (tp + fp) if (tp + fp) else 0.0
            recall = tp / (tp + fn) if (tp + fn) else 0.0
            accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else 0.0
            f1 = (
                (2 * precision * recall / (precision + recall))
                if (precision + recall)
                else 0.0
            )

            self._questions.at[qid, "precision"] = precision
            self._questions.at[qid, "recall"] = recall
            self._questions.at[qid, "f1_score"] = f1
            self._questions.at[qid, "accuracy"] = accuracy

    def _jaccard_similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        set1 = set(col1.index[col1 == "YES"])  # type: ignore
        set2 = set(col2.index[col2 == "YES"])  # type: ignore
        if not set1 and not set2:
            return 1.0
        union = set1 | set2
        return len(set1 & set2) / len(union) if union else 0.0

    def _hamming_similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        mask = col1.notna() & col2.notna()
        if not mask.any():
            return 0.0
        return float((col1[mask] == col2[mask]).mean())  # type: ignore

    def _similarity(self, col1: pd.Series, col2: pd.Series) -> float:
        return (
            self._jaccard_similarity(col1, col2)
            if self.answer_similarity_func == "jaccard"
            else self._hamming_similarity(col1, col2)
        )

    def filter_questions_on_pred_similarity(self, threshold: float | None) -> None:
        """Filter questions on prediction similarity.

        If two questions have a prediction similarity greater than or
        equal to the threshold, the question with the lower f1 score is excluded.

        Args:
            threshold: Threshold for prediction similarity. If None, no filtering is
            done based on prediction similarity.

        Raises:
            AssertionError: If threshold is not between > 0 and <= 1.
        """
        if threshold is not None:
            assert 0 < threshold <= 1, "Threshold must be between > 0 and <= 1"

        self._update_answers_df_columns()
        self._questions.loc[
            self._questions["exclusion"]
            == QuestionExclusion.PREDICTION_SIMILARITY.value,
            "exclusion",
        ] = None
        if threshold is None:
            return

        def _safe_f1(qid: str) -> float:
            if "f1_score" not in self._questions.columns:
                return 0.0
            val = cast(Any, self._questions.at[qid, "f1_score"])
            if isinstance(val, (int, float)):
                return float(val)
            if isinstance(val, np.generic):
                try:
                    return float(val.item())  # type: ignore[no-any-return]
                except Exception:
                    return 0.0
            return 0.0

        qids = cast(List[str], self._answers.columns.tolist())
        num_questions = len(qids)
        keep: List[bool] = [True] * num_questions

        for i in range(num_questions):
            if not keep[i]:
                continue
            col_i = cast(pd.Series, self._answers[qids[i]])
            for j in range(i + 1, num_questions):
                if not keep[j]:
                    continue
                col_j = cast(pd.Series, self._answers[qids[j]])
                sim = self._similarity(col_i, col_j)
                if sim >= threshold:
                    f1_i = _safe_f1(qids[i])
                    f1_j = _safe_f1(qids[j])
                    if f1_i < f1_j:
                        keep[i] = False
                        break
                    else:
                        keep[j] = False

        to_remove = [qids[k] for k in range(num_questions) if not keep[k]]
        if to_remove:
            self._questions.loc[to_remove, "exclusion"] = (
                QuestionExclusion.PREDICTION_SIMILARITY.value
            )

    async def _build_rrf(self) -> None:
        X = self._X
        y = self._y

        if X is None or y is None:
            raise ValueError("X, y,  must be set")

        logger.info("Generating questions")
        await self._generate_questions()
        logger.info(f"Generated {self._questions.shape[0]} questions")

        logger.info("Answering questions")
        await self._answer_questions()

        logger.info("Setting questions metrics")
        self._set_questions_metrics()

    def _set_data(self, X: pd.DataFrame, y: Sequence[str], copy_data: bool) -> None:
        if not all(isinstance(item, str) for item in y):  # type: ignore
            raise DataError("y must be a sequence of strings")

        if len(y) != X.shape[0]:
            raise DataError("y and X must have the same number of rows")

        if set(np.unique(y)) != {"YES", "NO"}:
            raise DataError(
                "y must be a sequence of only 'YES' or 'NO' values (case insensitive)"
            )

        y_array = np.array([yi.upper() for yi in y], dtype=np.str_)

        if copy_data:
            self._X = deepcopy(X).reset_index(drop=True)  # type: ignore
            self._y = deepcopy(y_array)
        else:
            self._X = X.reset_index(drop=True)  # type: ignore
            self._y = y_array

        self._questions = self._get_initial_questions_df()
        self._answers = self._create_answers_df(index=X.index)

    async def fit(
        self,
        X: pd.DataFrame | None = None,
        y: Sequence[str] | None = None,
        *,
        copy_data: bool = True,
        reset: bool = False,
    ) -> Self:
        """Fit the RRF to the data.

        Args:
            X: Training features. Required on first run or with reset=True.
            y: Training labels. Required on first run or with reset=True.
            copy_data: Whether to copy input data.
            reset: Clear existing state and restart forest generation.

        Returns:
            Self: Updated RRF.

        Raises:
            ValueError: If data requirements aren't met or invalid reset usage.
        """
        if reset:
            if X is None or y is None:
                raise ValueError("reset=True requires X and y")
            self._set_data(X, y, copy_data)
        else:
            if X is not None or y is not None:
                if self._X is not None or self._y is not None:
                    raise ValueError(
                        "Data already set on forest. Explicitly pass reset=True "
                        "to replace data and restart training."
                    )
                if X is None or y is None:
                    raise ValueError("Both X and y must be provided together")
                self._set_data(X, y, copy_data)

        if self._X is None or self._y is None:
            raise ValueError(
                "No data found on forest. Provide X and y (or reset=True with X,y)"
            )

        await self._build_rrf()
        logger.info("RRF built successfully")
        return self

    async def _predict_single(
        self,
        sample_index: int,
        sample: str,
        token_counter: TokenCounter,
    ) -> AsyncGenerator[Tuple[Any, str, Literal["YES", "NO"]], None]:
        """Predict a single sample data point."""
        for qid in cast(List[str], self._questions.index):
            if self._questions.at[qid, "exclusion"] is not None:
                continue

            answer = await self._answer_single_question(
                sample_index=sample_index,
                sample=sample,
                question_id=qid,
                question=cast(str, self._questions.at[qid, "question"]),
                token_counter=token_counter,
            )
            if answer is None:
                continue
            sample_index, _, answer = answer

            yield sample_index, qid, answer.answer

    async def predict(
        self, samples: pd.DataFrame
    ) -> AsyncGenerator[Tuple[Any, str, Literal["YES", "NO"], TokenCounter], None]:
        """Predict labels for samples.

        Args:
            samples: Samples to predict.

        Returns:
            Generator of predictions[sample_index, question, answer, token_counter]

        Raises:
            ValueError: If samples is empty or does not have the correct column.
        """
        queue: asyncio.Queue[
            Literal["DONE"] | Tuple[Any, str, Literal["YES", "NO"]]
        ] = asyncio.Queue()

        token_counter = TokenCounter()

        async def worker(sample_index: Any, sample: str) -> None:
            try:
                async for record in self._predict_single(
                    sample_index=sample_index,
                    sample=sample,
                    token_counter=token_counter,
                ):
                    await queue.put(record)
            finally:
                await queue.put("DONE")

        tasks: List[asyncio.Task[None]] = []
        try:
            for sample_index, row in samples.iterrows():
                sample_str = "\n".join([f"{col}: {val}" for col, val in row.items()])
                tasks.append(asyncio.create_task(worker(sample_index, sample_str)))

            remaining = len(tasks)
            while remaining > 0:
                item = await queue.get()
                if item == "DONE":
                    remaining -= 1
                    continue
                else:
                    yield item + (token_counter,)
            return
        except asyncio.CancelledError:
            pass
        finally:
            for task in tasks:
                if not task.done():
                    task.cancel()

    async def update_question_exclusion(
        self,
        question_id: str,
        exclusion: QuestionExclusion | None,
    ) -> str:
        """Update a question exclusion.

        Args:
            question_id: The id of the question to update.
            exclusion: The exclusion to set. If None, removes the exclusion status.

        Returns:
            str: The question that was updated.

        Raises:
            ValueError: If question id is not found.
        """
        if question_id not in self._questions.index:
            raise ValueError(f"Question with id {question_id} not found.")
        self._questions.at[question_id, "exclusion"] = (
            exclusion.value if exclusion else None
        )

        return (
            f"Updated exclusion of '{self._questions.at[question_id, 'question']}'"
            f" to {exclusion.value if exclusion else 'Not Excluded'}"
        )

    async def add_question(self, question: str) -> Literal[True]:
        """Add a question to the RRF.

        Args:
            question: The question to add.

        Raises:
            ValueError: If question already exists.
        """
        for qid, row in self._questions.iterrows():
            if row["question"].lower() == question.lower():  # type: ignore
                raise ValueError(f"Question '{question}' already exists with id {qid}.")

        id_ = str(int(max(self._questions.index)) + 1).zfill(3)  # type: ignore
        self._questions.at[id_, "question"] = question
        self._questions.at[id_, "exclusion"] = None

        logger.info("Answering question to data")
        await self._answer_questions()

        logger.info("Adding question metrics")
        self._set_questions_metrics()
        return True

    def save(
        self,
        dir_path: str | PathLike[str] | None = None,
        for_production: bool = False,
    ) -> None:
        """Save model config to JSON and dataframes to parquet in a directory.

        If dir_path is None, uses `<self.save_path>/<self.name>`.
        If for_production is True, strips the questions dataframe and does not save
        the answers and training dataframes.

        Args:
            dir_path: The directory to save the RRF to.
            for_production: Whether to save the RRF for production.
        """
        base = Path(dir_path) if dir_path is not None else (self.save_path / self.name)
        if base.is_file():
            raise ValueError("Please provide a directory, not a file.")
        base.mkdir(parents=True, exist_ok=True)

        if not for_production:
            qdf = self._questions.copy()
            if "embedding" in qdf.columns:
                qdf["embedding"] = qdf["embedding"].apply(  # type: ignore
                    lambda x: orjson.dumps(  # type: ignore
                        x.tolist() if isinstance(x, np.ndarray) else x
                    ).decode()
                    if x is not None
                    else None
                )
            qdf.to_parquet(base / "questions.parquet")  # type: ignore

            self._answers.to_parquet(base / "answers.parquet")  # type: ignore

            if sum([self._X is not None, self._y is not None]) == 1:
                raise CorruptionError(
                    "Model state is corrupted. Either X or y is missing."
                )
            if self._X is not None:
                df = deepcopy(self._X)
                df["y"] = self._y
                df.to_parquet(base / "data.parquet")  # type: ignore
        else:
            qdf = self._questions.copy()
            qdf = qdf.drop(columns=["embedding"])
            qdf.to_parquet(base / "questions.parquet")  # type: ignore

        manifest = {
            "name": self.name,
            "qgen_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.qgen_llmc
            ],
            "qanswer_llmc": [
                llmc if isinstance(llmc, dict) else llmc.model_dump()
                for llmc in self.qanswer_llmc
            ],
            "qgen_temperature": self.qgen_temperature,
            "qanswer_temperature": self.qanswer_temperature,
            "llm_semaphore_limit": self.llm_semaphore_limit,
            "answer_similarity_func": self.answer_similarity_func,
            "max_generated_questions": self.max_generated_questions,
            "max_samples_as_context": self.max_samples_as_context,
            "class_ratio": list(self.class_ratio),
            "q_answer_update_interval": self.q_answer_update_interval,
            "token_counter": None if for_production else self._token_counter.to_dict(),
            "save_path": str(self.save_path) if not for_production else None,
            "task_description": self._task_description,
            "last_emb_model": self._last_emb_model,
            "qgen_instructions_template": self._qgen_instructions_template
            if not for_production
            else None,
            "random_state": self.random_state,
        }
        with (base / "rrf.json").open("w", encoding="utf-8") as f:
            f.write(orjson.dumps(manifest).decode())

    @classmethod
    def _load(cls, dir_path: str | PathLike[str]) -> "RRF":
        """Load an RRF saved by `save`."""
        base = Path(dir_path)
        if base.is_dir():
            rrf_json_path = base / "rrf.json"
            if not rrf_json_path.exists():
                raise FileNotFoundError(f"'rrf.json' not found in directory: {base}")

            questions_path = base / "questions.parquet"
            if not questions_path.exists():
                raise FileNotFoundError(
                    f"'questions.parquet' not found in directory: {base}"
                )
        else:
            raise ValueError("Please provide a directory, not a file.")

        with rrf_json_path.open("r", encoding="utf-8") as f:
            manifest = orjson.loads(f.read())

        inst = cls(
            qgen_llmc=manifest["qgen_llmc"],
            qanswer_llmc=manifest["qanswer_llmc"],
            qgen_temperature=manifest["qgen_temperature"],
            qanswer_temperature=manifest["qanswer_temperature"],
            llm_semaphore_limit=manifest["llm_semaphore_limit"],
            answer_similarity_func=manifest["answer_similarity_func"],
            max_generated_questions=manifest["max_generated_questions"],
            max_samples_as_context=manifest["max_samples_as_context"],
            class_ratio=tuple(manifest["class_ratio"]),
            q_answer_update_interval=manifest["q_answer_update_interval"],
            random_state=manifest["random_state"],
            save_path=str(base.parent),
            name=manifest["name"],
        )

        inst._task_description = manifest["task_description"]
        inst._qgen_instructions_template = manifest["qgen_instructions_template"]
        inst._last_emb_model = manifest["last_emb_model"]
        if tk_dict := manifest["token_counter"]:
            inst._token_counter = TokenCounter.from_dict(tk_dict)

        inst._llm_semaphore = asyncio.Semaphore(manifest["llm_semaphore_limit"])

        q_path = base / "questions.parquet"
        qdf = pd.read_parquet(q_path)  # type: ignore
        if "embedding" in qdf.columns:
            qdf["embedding"] = qdf["embedding"].apply(  # type: ignore
                lambda x: np.array(orjson.loads(x), dtype=np.float32)  # type: ignore
                if x is not None
                else None
            )
        for col in [
            "excluded_in_semantics",
            "excluded_in_pred_similarity",
            "expert_excluded",
        ]:
            if col in qdf.columns:
                qdf[col] = qdf[col].astype(bool)  # type: ignore
        inst._questions = qdf

        a_path = base / "answers.parquet"
        if a_path.exists():
            inst._answers = pd.read_parquet(a_path)  # type: ignore

        training_data_path = base / "data.parquet"
        if training_data_path.exists():
            training_data = pd.read_parquet(training_data_path)  # type: ignore
            if "y" in training_data.columns:
                inst._y = training_data["y"].to_numpy(dtype=np.str_)  # type: ignore
                inst._X = training_data.drop(columns=["y"])  # type: ignore

        return inst

    @classmethod
    def load(cls, dir_path: str | PathLike[str]) -> "RRF":
        """Load an RRF saved by `save`."""
        try:
            return cls._load(dir_path)
        except KeyError as e:
            raise CorruptionError(
                f"Failed to load RRF. RRF json is probably corrupted: {e}"
            )
        except Exception as e:
            raise e

    def __repr__(self) -> str:
        return f"RRF(name={self.name})"

    def __str__(self) -> str:
        return f"RRF(name={self.name})"
